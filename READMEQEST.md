# QEST experiments

This file contains instructions of how to reproduce the experiments 
included in a paper (or at least its extended version) submitted to QEST.
There are three case studies in the paper, hence the instructions are
split into three parts.

A few things to note:
- All the commands below should be run from the top-level directory, the
same directory in which this README file is located.
- By default, the tool prints quite a lot of output when running 
simulations. To suppress this, append ```--log 1``` to the command
- I prepared and tested the VM image on a 2016 MBP running macOS Big
Sur (Version 11.1), with 8GB of RAM and 2.9 GHz Dual-Core Inter Core 
i5 processor. I also assigned 4GB of RAM to the VM in VirtualBox.

## Trust Game Experiments
Our trust game case study involves three hypotheses, first of which 
(H1) is somewhat trivial and can be confirmed along the second one.
Therefore, two experiments are needed to verify the hypotheses:

1. The first experiment investigates the relationship between initial 
trusts of agents and average transfers. Note that the value of trust
is determined by the value of belief (represented as parameters to a 
Dirichlet distribution). To run this experiment, execute the following
command:
```
webppl examples/trustgame/src/simulations.wppl --require . --require examples/trustgame -- --experiment 1 --log 1
```
(I included ```--log 1``` as otherwise A LOT of output is produced)
This might take 10 minutes or so as it simulates the execution twenty 
times
for each belief configuration and there are five different belief 
configurations. In the unlikely scenario that this takes too long to
run, passing ```--reps 10``` (or substitute 10 with an even smaller 
integer) will change the number of simulations of each configuration 
to 10.

The tool prints traces for each simulation, followed by average
investments and returns for each configurations. The hope is that
the averages will match Fig. 4 from the paper.

Disclaimer: I'll be honest, I have discovered a small bug in the 
implementation of Trust Game (state rewards were being assigned incorrectly)
after submitting the paper. This doesn't affect the simulations 
massively, but might affect them enough that the results of this 
experiment will be somewhat different.

2. The second experiment is about conman behaviour. To run it, execute
```
webppl examples/trustgame/src/simulations.wppl --require . --require examples/trustgame -- --experiment 3 --log 1
```
The tool will print traces from twenty (but ```--reps <int>``` can be 
passed to change that number) simulations along with the average 
income of Alice and Bob ('all 0' traces are ignored for the purposes
of computing averages).
This again takes 5-10 minutes to run (and prints plenty of cache info)
The key observation to make is that Bob's strategy is indeed quite
sophisticated - he cooperates initially, but he always keeps all
the money in the last round. He sometimes (hopefully that will be the
case but nothing can be guaranteed due to randomisation) manages to
deceive Alice, when she falls for his tricks and invests her full 
endowment on last round. Other times, Alice realises what he's up to
and she invests nothing on the last round. Overall, we expect Alice to
end up with higher average income, which partly reflects her priviledged
position in the game (she 'calls the shots') and partly the fact that
she doesn't let herself be manipulated by Bob, most of the time.

## Tipping Experiments

The second set of experiments is quite different in nature as it 
involves learning from data. Therefore, the process will be somewhat
more complex, but we provide scripts to automate it.
The experiments can be divided into two parts, first of which uses 
synthetic data, while the second uses data generated by simulations.

1. The first experiment involves inference from three batches of 
synthetic data, each characterised by low, medium or high tips. The data
files are in ```examples/tipping/data``` and to run inference, execute
```
bash examples/tipping/inferFromSyntheticData syntheticInfer.txt
```
This might take over 10 minutes (but hopefully less than 20) and 
it saves the results of inference to a file 
```syntheticInfer.txt``` in ```examples/tipping/results```. An 
alternative file name may be provided if needed. 
The results will be quite concise: for each inference (there will
be three of those), first comes prior, which is always the same 
(uniform), followed by posterior. The posteriors should roughly 
correspond to Figures 7 and 8 from the paper.

Additionally/optionally, we may perform an experiment briefly
mentioned in the paper where we fix Abi's goal coefficients and
her GASP score and infer only the tipping norm by executing
```
bash examples/tipping/inferFromSyntheticDataFixGoalCoeffsAndGasp syntheticInferFixed.txt
```
The results get saved in ```syntheticInferFixed.txt```. This experiment
should run significantly faster (as there's less inference) and the 
posterior should be characterised by less uncertainty.

Finally, to show that tipping norm cannot be accurately inferred from 
the data alone, without taking into account agent's characteristics, we
focus on ```mediumTips.csv```, the data file characterised by medium tips,
and we show that for various configurations of Abi's goal coefficients
and GASP score, inferred posterior on tipping norm will differ.
To perform this experiment, execute:
```
bash examples/tipping/inferTippingNormFromMediumTips syntheticInferTippingNorm.txt
```
(It may take up to 10 minutes)
This script randomises Abi's goal coefficients and her GASP score ten 
times and infers posterior on tipping norm. A more accurate prior is used 
since only one type of data is considered. Hopefully, the posteriors
will be different. 

2. The second type of experiments involves learning from data generated 
by the tool. We start by generating data:
```
bash examples/tipping/generateDataFiles
```
This should create a subdirectory ```generatedData``` within the 
```data``` directory and inside it, three subdirectories ```5```,
```10``` and ```15```, corresponding to the number of rounds of
tipping game simulated. 

To learn from the generated files, execute the following three commands:
```
bash examples/tipping/inferFromGeneratedData inferFromGenerated5rounds.txt 5 10
bash examples/tipping/inferFromGeneratedData inferFromGenerated10rounds.txt 10 10
bash examples/tipping/inferFromGeneratedData inferFromGenerated15rounds.txt 15 10
```

This will likely take a while, especially the last command (1-2 hours).
Each command processes ten files and prints its progress to the user which
should give one an idea of how long exactly it should take.
It will save the results of inference to the above files and it is important
to use these exact filenames as they are hardcoded in the evaluation script
(see below, it's not an ideal solution but it works...). 

To evaluate the posteriors, we have included a python script which reads
the text files (both data files and results file) to obtain real values
and predictions and computes PMSE (generalisation of MSE to predictions)
for each prediction (separately for goal coefficients, tipping norm and 
gasp score). PMSEs are then averaged, as means and medians and printed
to the user. 
Execute the following to evaluate the predictions:
```
python3 util/computePMSE.py 5 10
python3 util/computePMSE.py 10 10
python3 util/computePMSE.py 15 10
```
First argument specifies number of rounds of data (which allows the script
to locate the right directory inside ```generatedData``) while the second
argument specifies how many files are in that directory.
The results will hopefully match Table 2 from the paper.

## Bravery Game Experiments
Bravery game experiments are arguably the most trivial of all and 
consist of two simple experiments:

1. The first experiment sets parameters of agents to reflect the 
psychological game model, but varies beliefs of player 1 and investigates
whether an equilibrium is reached. 
To run, execute
```
webppl examples/bravery/src/simulations.wppl --require . --require examples/bravery -- --experiment 1 --log 1
```
The result will be in the form of three traces (for different values of 
initial beliefs) - these have to be inspected manually. The paper makes
a claim that an equilibrium is reached regardless of initial belief -
this will be confirmed if the traces are dominated by *bold* actions
from some point onwards. 

2. The second experiment studies how p1's lookahead affects the total
reward in the game. To run it, execute:

```
webppl examples/bravery/src/simulations.wppl --require . --require examples/bravery -- --experiment 2 --runs 5 --log 1
```
This prints total rewards of players accumulated over 10 rounds for three
different values of lookahead (1,3,5), averaged over five runs. The results
should roughly match the values in Table 3. 
