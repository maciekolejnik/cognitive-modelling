/** Tipping game
 *  This scenario models cognitive process that underlies humans
 *  giving tips. The idea is that deciding on the tip amount involves
 *  a tradeoff between money and guilt. Tipping less saves one ('tipper')
 *  money but increases the feeling of guilt resulting from disappointing
 *  expectation of the 'tipee'.
 *
 *  Therefore, the utility function will consist of one physical reward
 *  (money) and one mental reward (guilt). We expect guilt to be
 *  inversely proportional to the tip amount, but also dependent on
 *  the quality of service received and cultural norms. Eg. tipping
 *  an averagely performing waiter in America 15% might be considered
 *  low and lead to high degree of guilt, while in some parts of Europe
 *  a 10% tip would be appropriate when excellent service was received
 *  and result in no guilt. Those norms are captured by a parameter of
 *  our model, *tippingNorm*, which takes a percentage value, usually
 *  between 0% and 20%. Each individual also has a unique proneness
 *  to guilt, which we represent via a (latent) parameter, gaspScore, of
 *  each agent.
 *
 *  We consider a simple one-shot scenario, with two agents that we refer
 *  to as 'tipper' (person giving the tip, aka player 1) and 'tipee'
 *  (person providing some service and receiving a tip, aka player 0).
 *  We assume that tipee moves first and their action captures the
 *  quality of service they provided (be it waiting, taxi ride,
 *  hotel consierge etc). For simplicity,
 *  we consider three different actions/QoS - bad, normal, good.
 *  Following receiving the service, we allow tips with various
 *  amounts: 0%, 5%, 10%, 15%, 20% and 25%. This interaction
 *  may then repeat as needed.
 *
 *  State is represented as usual by capturing the history of
 *  play so far, in particular an object of the form
 *  {
 *      pastService: <array of QoS action, most recent at index 0>,
 *      pastTips: <array of tip amounts, most recent at index 0>
 *  }
 *
 *
 */

/**
gameSpecificParams = {
    tippingNorm: <percentage value eg 5%,10%,20%>,
    gaspScores: <array giving GASP (0-7) for each agent, eg [4.57, 5.89]>
}
 */
let makeTippingCSMG = function(gameSpecificParams) {
    /** retrieve params for easier access.
     * make sure they're in the expected format */
    let tippingNorm = gameSpecificParams.tippingNorm
    let gaspScores = gameSpecificParams.gaspScores

    assertHasType(tippingNorm, INT_TYPE, "tipping norm must be passed and be an integer")
    assertIsArray(gaspScores, NUMBER_TYPE, 2, "gaspScores array must be passed with two scores")

    /**
     * Game-specific auxiliary functions, constants etc
     */

    let TIPEE = 0
    let TIPPER = 1


    /**
     * Now come functions that describe the mechanics of the game
     */

    /** Actions available to an agent (the owner of *state*
     * - assumed to be unique) in *state* */
    let actions = function(state) {
        let actionsByPlayer = [['bad', 'normal', 'good'], [0,5,10,15,20,25,30]]
        return actionsByPlayer[turn(state)]
    }

    let transitionFn = function(state, action) {
        if (turn(state) == TIPEE) {
            return extend(state, { pastService: [action].concat(state.pastService) })
        } else {
            return extend(state, { pastTips: [action].concat(state.pastTips) })
        }
    }

    /**
     * Now comes the "API" of the game.
     * This is a set of functions which are used by our library but
     * whose implementation is game-specific.
     * */

    /** returns whose turn it is to take an action at *state*.
     *  this is also referred to as the owner of that state.
     *  players are indentified by nonnegative integer */
    let turn = function (state) {
        if (state.pastService.length > state.pastTips.length) {
            /** It's tipper's turn */
            return 1
        } else {
            return 0
        }
    }

    /** return action that was taken to get to *state* */
    let getLastAction = function (state) {
        assert(!isInitial(state), "Calling previousAction on initial state")
        return (turn(state) == TIPPER) ? state.pastService[0] : state.pastTips[0]
    }

    let API = function() {
        /** return state which preceded *state* in the game, or *state* if it is initial state.
         * If called on initial state, this function can return whatever */
        let getPreviousState = function(state) {
            if (isInitial(state)) return state
            let lastTurn = other(turn(state))
            let pastService = state.pastService
            let pastTips = state.pastTips
            let prevPastService = (lastTurn == TIPEE) ? pastService.slice(1) : pastService
            let prevPastTips = (lastTurn == TIPPER) ? pastTips.slice(1) : pastTips
            return {
                pastService: prevPastService,
                pastTips: prevPastTips
            }
        }

        /** checks whether *state* is the initial state */
        let isInitial = function (state) {
            return state.pastService.length == 0 && state.pastTips.length == 0
        }


        /** retrieves the other player (we currently assume there
         * are only 2 players). normally, it's recommended that
         * players are represented by contiguous nonnegative integers
         * i.e. 0 and 1 - the default implementation below (could be
         * overriden) */
        let other = function (player) {
            return (player + 1) % 2
        }

        /** returns the string representation of *state*.
         * (for debugging purposes) */
        let stateToString = function (state) {
            return "pastService: " + arrayToString(state.pastService) +
                ",\npastTips: " + arrayToString(state.pastTips)
        }

        let API = {
            getPreviousState,
            getLastAction,
            isInitial,
            turn,
            other,
            stateToString
        }

        return API
    }()



    /**
     *  Now comes physical reward structure.
     *  This should typically be a simple component capturing what,
     *  and how much, physical rewards (money, time, number of sweets
     *  etc) agents receive in each state
     */
    let physicalRewardStructure = function() {

        /** Physical rewards gained by each agent at *state*
         * Should return an array indexed by agentID
         */
        let stateRewards = function(state) {
            return [[0], [0]]
        }

        /** As above but for action rewards
         */
        let actionRewards = function(state, action) {
            if (turn(state) == TIPPER) return [[action], [-action]]
            return [[0],[0]]
        }

        return {
            actionRewards,
            stateRewards,
            quantity: 1
        }
    }()

    /**
     *  Now comes mental state dynamics model. That's the most important
     *  component, it captures the mental quantities (such as trust, guilt,
     *  pleasure, fairness, reciprocity).
     *  It consists of two components:
     *  (i) Heuristics agents use to estimate mental state of their opponents.
     *    This should be specified as an array of update functions, one for each
     *    mental state. Each update function has
     *    @type (mentalStateValue, estimatingAgent, estimatedAgent, state, action) -> newMentalStateValue
     *  (ii) Mental state computation, i.e. how can actual mental state of an
     *    agent be computed. Each such function that computes some mental state
     *    of an agent has
     *    @type (state, belief) -> mentalStateValue
     */
    let getMentalStateDynamics = function(beliefAPI) {

        /** We're concerned with the way agent perceives their own guilt
         *  so no need for estimating */
        let updateGuiltEstimation = function(guilt, estimatingAgent, estimatedAgent, state, action) {
            return guilt
        }

        /** how does tipper "compute" guilt from tipping?
         *  note 'compute' is used informally as it would be
         *  more appropriate to say 'feel', 'perceive' etc.
         *  since our model operates with rewards rather than costs
         *  guilt will be negative (and the more guilty an agent
         *  feels, the lower its value will be, bounded by -1).
         *  We also allow positive values of guilt which reflect
         *  agent feeling good about themselves when giving
         *  'higher than expected' tip.
         *  */
        let computeGuilt = function(state, agent, belief) {
            /** compute reference value, which specifies expected
             * tip given tipping norm and QoS received. */
            if (agent == TIPEE) return 0
            if (turn(state) == TIPPER) return 0
            if (isInitial(state)) return 0
            let ref = function() {
                let table = {
                    'bad': tippingNorm * 0.8,
                    'normal': tippingNorm,
                    'good': tippingNorm * 1.2
                }
                return table[state.pastService[0]]
            }()
            assertHasType(ref, INT_TYPE, "gulit reference value not a number: " + ref)
            assert(ref > 0, "tipping reference value must be >0")
            let turn = turn(state)
            /** only tipper is guilty after their tip */
            let lastTip = getLastAction(state)
            assertHasType(lastTip, NUMBER_TYPE,
                "last action when computing guilt must be a tip, found: " + lastTip)
            /** guilt is modeled using log function appropriately
             * scaled according to the value of gasp score
             */
            display("compute guilt at " + stateToString(state))
            let scale = 3/2
            let coeff = 1 - Math.exp(-scale)
            let tipProp = (lastTip - ref) / ref
            let gasp = gaspScores[TIPPER] / 7
            let guilt = Math.log(tipProp * coeff + 1) / scale * gasp
            assertHasType(guilt, NUMBER_TYPE,
                "computed guilt should be a number, found: " + guilt)
            assert(guilt <= 1 && guilt >= -1,
                "guilt should be between -1 and 1, found: " + guilt)
            return guilt
        }

        return {
            estimationHeuristicArr: [ updateGuiltEstimation ],
            mentalStateArr: [ computeGuilt ]
        }
    }

    /** Defines the initial state of the model (we assume there's only one such) */
    let initialState = {
        pastService: [],
        pastTips: []
    }

    /** belief representation and number of rewards (total = physical + mental)
     * must be specified */
    let params = {
        beliefRepresentation: 'dirichlet' /** or 'discrete' */,
        numberOfRewards: 2
    }

    /** Those are functions that make up the utility function that are
     * applied to each reward to possibly modify its value */
    let rewardUtilityFunctions = function() {
        let moneyUtility = identity

        let guiltUtility = function(x) {
            return 10 * x
        }

        return [moneyUtility, guiltUtility]
    }()

    return {
        actions,
        transitionFn,
        initialState,
        physicalRewardStructure,
        getMentalStateDynamics,
        rewardUtilityFunctions,
        API,
        params
    }
}