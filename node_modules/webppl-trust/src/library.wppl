let getGameAPI = function (gameSpecificAPI) {

  let stateToString = gameSpecificAPI.stateToString

  let mentalSnapshotToString = function (mentalSnapshot) {
    let state = stateToString(mentalSnapshot.state)
    let values = mentalSnapshot.values
    return "{state: " + state + ", values: " + values + "}"
  }

  /** This implements default action similarity measure (which we could
   * call 'discrete measure') in case a measure is not provided in
   * user game specification */
  let actionSimilarity = function(a1, a2) {
    let actionSimilarityOpt = gameSpecificAPI.actionSimilarity
    if (actionSimilarityOpt !== undefined) return actionSimilarityOpt(a1,a2)
    if (_.isEqual(a1,a2)) return 1
    return 0
  }

  let APIExtension = {
    mentalSnapshotToString,
    actionSimilarity
  }

  let gameAPI = extend(gameSpecificAPI, APIExtension)
  return gameAPI
}


/** makeCSMG (cognitive stochastic multiplayer game)
 * @param gameSetup
 *  an object of the form
 *  {
    actions,
    transition,
    initialState,
    API,
    getPhysicalRewards,
    getMentalStateDynamics,
    utilityFn,
    params
    }
 * This object contains all the game specific information needed
 * to create a CSMG and it is defined by the user.
 *
 * Given a game configuration, this generic function creates a
 * cognitive model. This mostly involves creating an appropriate
 * mental reward structure.
 */
let makeCSMG = function (gameSetup, externalParams) {
  /** Extract basic components */
  assertDefined(gameSetup,
    "makeCSMG(): gameSetup undefined")
  assertDefined(externalParams,
    "makeCSMG(): externalParams undefined")
  let params = assertDefined(gameSetup.params,
    "makeCSMG(): gameSetup.params undefined")
  let beliefRepresentation = assertDefined(externalParams.beliefRepresentation,
    "makeCSMG(): params.beliefRepresentation undefined")
  let numberOfAgents = assertDefined(params.numberOfAgents,
    "makeCSMG(): params.numberOfAgents undefined")
  let numberOfRewards = assertDefined(params.numberOfRewards,
    "makeCSMG(): params.numberOfRewards undefined")
  assertDefined(numberOfRewards.physical,
    "makeCSMG(): numberOfRewards.physical undefined")
  assertDefined(numberOfRewards.mental,
    "makeCSMG(): numberOfRewards.mental undefined")

  let actions = assertDefined(gameSetup.actions,
    "makeCSMG(): gameSetup.actions undefined")
  let transitionFn = assertDefined(gameSetup.transitionFn,
    "makeCSMG(): gameSetup.transitionFn undefined")
  let initialState = assertDefined(gameSetup.initialState,
    "makeCSMG(): gameSetup.params undefined")

  /** Extract game-specific API calls for easy access */
  let API = getGameAPI(gameSetup.API)

  let getPreviousState = assertDefined(API.getPreviousState,
    "makeCSMG(): API.getPreviousState undefined")
  let getLastAction = assertDefined(API.getLastAction,
    "makeCSMG(): API.getLastAction undefined")
  let isInitial = assertDefined(API.isInitial,
    "makeCSMG(): API.isInitial undefined")
  let stateToString = assertDefined(API.stateToString,
    "makeCSMG(): API.stateToString undefined")

  /** Reward structures */
  let physicalRewardStructure = assertDefined(gameSetup.physicalRewardStructure,
    "makeCSMG(): gameSetup.physicalRewardStructure undefined")
  let mentalStateDynamics = assertDefined(gameSetup.mentalStateDynamics,
    "makeCSMG(): gameSetup.mentalStateDynamics undefined")
  let estimationHeuristicsArr = assertDefined(mentalStateDynamics.estimationHeuristicArr,
    "getMentalRewardStructure(): estimationHeuristicsArr undefined")
  let mentalStateArr = assertDefined(mentalStateDynamics.mentalStateArr,
    "getMentalRewardStructure(): mentalStateArr undefined")
  let mentalUtilities = assertDefined(mentalStateDynamics.mentalUtilities,
    "getMentalRewardStructure(): mentalUtilities undefined")
  assertIsArray(estimationHeuristicsArr, FUNCTION_TYPE, numberOfRewards.mental,
    "getMentalRewardStructure: estimationHeuristicArr: " + arrayToString(estimationHeuristicsArr))
  assertIsArray(mentalStateArr, FUNCTION_TYPE, numberOfRewards.mental,
    "getMentalRewardStructure: mentalStateArr: " + arrayToString(mentalStateArr))
  assertIsArray(mentalUtilities, ARRAY_TYPE, numberOfAgents,
    "getMentalRewardStructure: mentalUtilities: " + arrayToString(mentalUtilities))

  let goalCoeffsNumberByAgent =
    computeGoalCoeffsNumber(numberOfRewards.physical, mentalUtilities)


  /** Reward Utilities */
  let rewardUtilityFunctions = assertDefined(gameSetup.rewardUtilityFunctions,
    "makeCSMG(): gameSetup.rewardUtilityFunctions undefined")

  /** Physical rewards structure
   *  This simply delegates to the physicalRewards object as given
   *  in gameSetup
   */
  let getPhysicalRewardStructure = function () {
    return physicalRewardStructure
  }

  /** Mental reward structure uses mental state dynamics provided in
   * @gameSetup to enable computation of mental rewards in any state.
   * Primarily, this is used by an agent to estimate mental state of
   * their opponent.
   * However, it is also used by an agent to estimate their own mental
   * state at some future point, thereby not requiring to compute belief
   * of that agent.
   *
   * @param initialEstimations (@type array of array of distributions,
   * with holes, indexed by agent)
   *  initial mental state estimations of this agent
   * @param selfAgentID
   *  identifier (nonnegative integer, usually 0 or 1) of an agent
   *  identifies the agent for which this reward structure is provided
   */
  let getMentalRewardStructure = function (initialEstimations, selfAgentID) {
    /**
     * @type array of update functions (each of @type (value, agent, state, action) -> newValue)
     */

    /** Represents @selfAgentID computing mental rewards of @ofAgentID.
     * There are four cases:
     * 1) agent is computing their own mental rewards (agentID = ofAgentID)
     * 2) agent is computing other agent's mental rewards (agentID != ofAgentID)
     *
     * a) value of self mental state is a reward
     * b) value of other's mental state is a reward
     *
     * There are two cases:
     * - if @mentalSnapshot is missing, @agentID is computing its own mental rewards
     *   as part of selecting its best action (expected utility)
     * - if @mentalSnapshot is passed, @agentID is computing its opponent's mental
     *   rewards, either as part of selecting an action or updating belief
     * @param state
     * @param mentalSnapshot (optional)
     *  a reference point from which to use mental state dynamics
     *  it's an object { state, values }
     *  (it snapshots mental state values @estimations at @state)
     *  if omitted, initial state with initial estimations serves as snapshot
     */
    let computeMentalRewards = function (state, ofAgentID, belief, mentalSnapshot) {
      info("computeMentalRewards(): " + selfAgentID + " computing men rews of " +
        ofAgentID)
      // let estimationsDists = computeMentalStateEstimations(state, other(selfAgentID))
      // let estimations = map(expectation, estimationsDists)
      // info("estimations (expns) computed: " + arrayToString(estimations))
      let mentalState = computeMentalState(state, belief)
      info("computeMentalRewards(): mental state computed: " + arrayToString(mentalState))
      /** ofAgentMentalUtility is an array with one elem per mental attitude
       * and that elem is an array which identifies agents whose attitude
       * (this particular one) ofAgentID cares about */
      let ofAgentMentalUtility = mentalUtilities[ofAgentID]
      if (selfAgentID == ofAgentID) {
        info("computeMentalRewards(): " + selfAgentID + " computing their own rewards")
        let estimateRewardsFromAttitude = function (mentalAttitudeIndex, agentArr) {
          let estimateReward = function(overAgentID) {
            if (overAgentID === selfAgentID) return mentalState[mentalAttitudeIndex]
            return expectation(estimation(state, overAgentID, mentalAttitudeIndex))
          }
          map(estimateReward, agentArr)
        }
        let rewardsByAttitudeArr = mapIndexed(estimateRewardsFromAttitude, ofAgentMentalUtility)
        info("computeMentalRewards(): about to join " + arrayToString(rewardsByAttitudeArr))
        return arrayConcat(rewardsByAttitudeArr)
      } else {
        /** now selfAgentID is computing (estimation) ofAgentID's mental rewards
         * this is nested reasoning and we simplify it by assuming that selfAgentID
         * uses
         * * his mental state estimations to estimate ofAgentID's own mental state
         * * estimation from snapshot to estimate ofAgentID's estimations of
         * selfAdgentID's mental state
         * * his own estimations to estimate ofAgentID's estimation of other
         * agents' mental state
         */
        info("computeMentalRewards(): " + selfAgentID + " computing " +
          ofAgentID + "\'s rewards")
        let estimateRewardsFromAttitude = function (mentalAttitudeIndex, agentArr) {
          /** i'th mental attitude, agentArr identifies agents whose
           * mental state we care about */
          let estimateReward = function(overAgentID) {
            /** j identifies agent whose mental state we're trying to
             * estimate */
            if (overAgentID === selfAgentID) {
              return nestedEstimation(state, mentalAttitudeIndex, mentalSnapshot)
            }
            return expectation(estimation(overAgentID, mentalAttitudeIndex))
          }
          return map(estimateReward, agentArr)
        }
        let rewardsByAttitudeArr = mapIndexed(estimateRewardsFromAttitude, ofAgentMentalUtility)
        info("computeMentalRewards(): about to join " + arrayToString(rewardsByAttitudeArr))
        return arrayConcat(rewardsByAttitudeArr)
      }
      // if (mentalSnapshot !== undefined) {
      //   /** compute opponent's mental rewards */
      //   return computeMentalRewardRecFromSnapshot(state, mentalSnapshot)
      // }
      // /** compute own mental rewards */
      // let estimations = computeMentalStateEstimations(state)
      // let estimationExpectation = function(estimation) {
      //   return expectation(estimation)
      // }
      // let result = map(estimationExpectation, estimations)
      // return result
    }

    /** returns array of values of mental states */
    let computeMentalState = function (state, belief) {
      info("computeMentalState(): state=" + stateToString(state) +
      ", belief: " + beliefToString(belief, goalCoeffsNumberByAgent))
      let f = function (mentalStateComputeFn) {
        return mentalStateComputeFn(state, belief)
      }
      return map(f, mentalStateArr)
    }

    /** returns the estimated value of this agent's mental state (identified
     * by *mentalAttitudeIndex*), computed using mental state dynamics
     * relative to a true value at some past state saved in *mentalSnapshot*
     * */
    let nestedEstimation = function(state, mentalAttitudeIndex, mentalSnapshot) {
      /** Local recursive function to save stack space by passing less parameters */
      let nestedEstimationRec = function(state) {
        if (_.isEqual(state, snapState)) {
          return snapValues[mentalAttitudeIndex]
        }
        let prevState = getPreviousState(state)
        let lastAction = getLastAction(state)
        let prevValue = nestedEstimationRec(prevState)
        let curValue = updateFn(prevValue, selfAgentID, prevState, lastAction)
        return curValue
      }
      assertDefined(state,
        "nestedEstimation(): state undefined!")
      assertDefined(mentalAttitudeIndex,
        "nestedEstimation(): mentalAttitudeIndex undefined!")
      assertDefined(mentalSnapshot,
        "nestedEstimation(): mentalSnapshot undefined!")
      let snapState = mentalSnapshot.state
      let snapValues = mentalSnapshot.values
      let updateFn = estimationHeuristicsArr[mentalAttitudeIndex]
      return nestedEstimationRec(state)
    }

    // let computeMentalRewardRecFromSnapshot = dp.cache(function (state, mentalSnapshot) {
    //   assertDefined(mentalSnapshot,
    //     "mentalRewardRecFromSnapshot: Mental snapshot undefined!")
    //   let snapState = mentalSnapshot.state
    //   let snapValues = mentalSnapshot.values
    //   if (_.isEqual(state, snapState)) {
    //     return snapValues
    //   }
    //   let prevState = getPreviousState(state)
    //   let lastAction = getLastAction(state)
    //   let prevValues = computeMentalRewardRecFromSnapshot(prevState, mentalSnapshot)
    //   let updateValue = function (updateFn, prevValue) {
    //     return updateFn(prevValue, selfAgentID, prevState, lastAction)
    //   }
    //   let curValues = map2(updateValue, estimationHeuristicsArr, prevValues)
    //   return curValues
    // })

    /** computes this agent's (selfAgentID) estimation of *ofAgentID*
     * mental state (attitude *rewardIndex*) */
    let estimation = dp.cache(
      function(state, ofAgentID, rewardIndex) {
        info("estimation(): state=" + stateToString(state) +
          ", ofAgentID=" + ofAgentID + ", rewardIndex=" + rewardIndex)
        if (isInitial(state)) {
          assert(initialEstimations[ofAgentID] !== undefined &&
            initialEstimations[ofAgentID][rewardIndex] !== undefined,
            "initial estimations of agent " + selfAgentID +
          " about " + ofAgentID + " on mental reward " + rewardIndex +
            " requested, but undefined")
          return initialEstimations[ofAgentID][rewardIndex]
        }
        let prevState = getPreviousState(state)
        let lastAction = getLastAction(state)
        let prevEstimation = estimation(prevState, rewardIndex, ofAgentID)
        let updateEstimation = function (updateFn, prevEstimation) {
          return Infer({method: 'enumerate'}, function () {
            let prevValue = sample(prevEstimation)
            return updateFn(prevValue, ofAgentID, prevState, lastAction)
          })
        }
        // let curEstimations = map2(updateEstimation, estimationHeuristicsArr, prevEstimations)
        return updateEstimation(estimationHeuristicsArr[rewardIndex], prevEstimation)
    })

    // let computeMentalStateEstimations = dp.cache(function (state, ofAgentID) {
    //   if (isInitial(state)) return initialEstimations[ofAgentID]
    //   let prevState = getPreviousState(state)
    //   let lastAction = getLastAction(state)
    //   let prevEstimations = computeMentalStateEstimations(prevState, ofAgentID)
    //   let updateEstimation = function (updateFn, prevEstimation) {
    //     return Infer({method: 'enumerate'}, function () {
    //       let prevValue = sample(prevEstimation)
    //       return updateFn(prevValue, other(selfAgentID), prevState, lastAction)
    //     })
    //   }
    //   let curEstimations = map2(updateEstimation, estimationHeuristicsArr, prevEstimations)
    //   return curEstimations
    // })

    return {
      computeMentalRewards,
      computeMentalState
    }
  }

  let utilityFn = function (goalCoeffs, rewards) {
    info("utilityFn(): goalCoeffs: " + arrayToString(goalCoeffs) +
      ", rewards: " + rewards)
    assertEqual(goalCoeffs.length, rewards.length,
      "Dimension of goal coefficients= " + goalCoeffs.length +
      " doesn't match rewards= " + rewards.length)
    let rewardUtilities = map2(apply, rewardUtilityFunctions, rewards)
    let result = sum(map2(multiply, goalCoeffs, rewardUtilities))
    info("utility computed: " + result)
    return result
  }

  let updatedParams = extend(params, {
    goalCoeffsNumberByAgent,
    beliefRepresentation
  })

  return {
    actions,
    transitionFn,
    initialState,
    API,
    getPhysicalRewardStructure,
    getMentalRewardStructure,
    utilityFn,
    params: updatedParams
  }
}

/** Cognitive trust aware agent

 * As described in the paper, an agent is characterised by
 * 1. an array of coefficients
 *    *goalCoeffs* = [a1,a2,...,an]
 *    (we require a1 + a2 + ... + an = 1)
 * 2. meta-parameters
 *    *metaParams* = {
 *      alpha: [0,inf),
 *      lookAhead: [0, inf) (integer),
 *      discountFactor: (0,1]
 *    }
 * The above dictionaries are stored in a single dict *selfParams*
 * Moreover, each agent has a role (*selfId*), which is 'investor' or 'investee'
 * Next, each agent has an initial state, consisting of
 * 3. initial belief about opponent's mental characteristics.
 *   Currently assumed to be Dirichlet distribution and represented by
 *   its set of parameters. However, implementation should be representation-agnostic:
 *   we provide certain operations on belief:
 *   - updateBelief
 * 4. initial estimations of mental rewards of agent's opponent.
 *    At the minimum, this contains estimation of trust.
 *    It is an array.
 *    Eg [<trustEstimation:distribution>]
 * 5. estimation of opponent's mental parameters
 *    metaParamsEstimations = distribution over values
 {
        alpha: <discrete dist>,
        lookAhead: <discrete dist>,
        discountFactor: <discrete dist>
      }
 Hence, *initialState* takes a form of a following dictionary {
     belief: <3>,
     mentalEstimations: <4>,
     metaParamsEstimations: <5>
   }
 *
 */
// TODO: could be nice here if gamespecific api was allowed to assert params
//  passed are valid (eg selfid needs to be investor of investee)
let makeAgent = function (selfParams, selfId, initialState, game) {
  /** Extract some functions from the game API for easy access */
  let gameAPI = assertDefined(game.API, "makeAgent(): game.API undefined ")
  let isInitial = assertDefined(gameAPI.isInitial,
    "makeAgent(): gameAPI.isInitial undefined")
  let getPreviousState = assertDefined(gameAPI.getPreviousState,
    "makeAgent(): gameAPI.getPreviousState undefined")
  let getLastAction = assertDefined(gameAPI.getLastAction,
    "makeAgent(): gameAPI.getLastAction undefined")
  let stateToString = assertDefined(gameAPI.stateToString,
    "makeAgent(): gameAPI.stateToString undefined")
  let turn = assertDefined(gameAPI.turn,
    "makeAgent(): gameAPI.turn undefined")
  let actionSimilarity = assertDefined(gameAPI.actionSimilarity,
    "makeAgent(): gameAPI.actionSimilarity undefined")

  /** Extract some fields for easier access */
  let transitionFn = game.transitionFn
  let actions = game.actions
  let utilityFn = game.utilityFn

  let gameParams = assertDefined(game.params,
    "makeAgent(): game.params undefined")
  let beliefRepresentation = gameParams.beliefRepresentation
  let numberOfAgents = gameParams.numberOfAgents
  let numberOfRewards = gameParams.numberOfRewards
  let goalCoeffsNumberByAgent = gameParams.goalCoeffsNumberByAgent

  assertDefined(selfId, "makeAgent(): selfId not given")
  /** Meta-parameters */
  assertDefined(selfParams, "makeAgent: selfParams not passed")
  let selfMetaParams = assertDefined(selfParams.metaParams,
    "makeAgent: metaParams missing")
  assertDefined(selfMetaParams.lookAhead, "makeAgent: lookAhead missing")
  assertDefined(selfMetaParams.alpha, "makeAgent: alpha missing")
  assertDefined(selfMetaParams.discountFactor, "makeAgent: discountFactor missing")

  /** Initial state */
  assertDefined(initialState, "makeAgent: initialState missing")
  let initialBeliefValue = assertDefined(initialState.belief,
    "makeAgent: initialBelief missing")
  let initialBelief = {
    representation: beliefRepresentation,
    value: initialBeliefValue
  }
  let initialMentalEstimations = assertDefined(initialState.mentalEstimations,
    "makeAgent: initialMentalEstimations missing")
  let metaParamsEstimations = assertDefined(initialState.metaParamsEstimations,
    "makeAgent: metaParamsEstimations missing")
  assertDefined(metaParamsEstimations.alpha,
    "makeAgent: metaParamsEstimations.alpha missing")
  assertIsArray(metaParamsEstimations.alpha, OBJECT_TYPE, numberOfAgents,
    "makeAgent: metaParamsEstimations.alpha is not as expected: ")
  assertDefined(metaParamsEstimations.lookAhead,
    "makeAgent: metaParamsEstimations.lookAhead missing")
  assertIsArray(metaParamsEstimations.lookAhead, OBJECT_TYPE, numberOfAgents,
    "makeAgent: metaParamsEstimations.lookAhead is not as expected: ")
  assertDefined(metaParamsEstimations.discountFactor,
    "makeAgent: metaParamsEstimations.discountFactor missing")
  assertIsArray(metaParamsEstimations.discountFactor, OBJECT_TYPE, numberOfAgents,
    "makeAgent: metaParamsEstimations.discountFactor is not as expected: ")

  /** Reward structures */
  let getPhysicalRewardStructure = game.getPhysicalRewardStructure
  let physicalRewardStructure = assertDefined(getPhysicalRewardStructure(),
    'physicalRewardStructure undefined')
  let stateRewards = physicalRewardStructure.stateRewards
  let actionRewards = physicalRewardStructure.actionRewards

  let getMentalRewardStructure = game.getMentalRewardStructure
  let mentalRewardStructure = assertDefined(getMentalRewardStructure(initialMentalEstimations, selfId),
    'mentalRewardStructure undefined')
  let computeMentalRewards = mentalRewardStructure.computeMentalRewards
  let computeMentalState = mentalRewardStructure.computeMentalState

  /** Goal coefficients of this agent */
  let selfGoalCoeffs = selfParams.goalCoeffs
  let selfPhysicalGoalCoeffs = selfGoalCoeffs.slice(0, numberOfRewards.physical)
  let selfMentalGoalCoeffs = selfGoalCoeffs.slice(numberOfRewards.physical)

  // console.assert(selfGoalCoeffs.length == initialMentalEstimations.length + ??)


  /*********************
   * Belief operations *
   *********************
   Abstractly, belief is a continuous probability distribution.
   Concretely, two finite representations are proposed:
   - a discrete distribution that approximates a continuous distribution
   - a set of parameters of dirichlet distribution; here we make an
   assumption that belief follows dirichlet dist
   We would like to be able to change between those, and possibly more,
   representations easily, while exposing a uniform interface for the
   rest of the code that uses belief.
   We therefore include the following methods in the 'interface' of belief
   - belief(state)
   - updateBelief(belief, observation)
   */

  /** Belief
   *  Computes selfID's belief of ofAgentID in *state*. Proceeds recursively.
   *  To avoid recomputation, this function is cached.
   */
  let belief = dp.cache(function (state) {
    info("belief(): state: " + stateToString(state) + ", of agent " + selfId)
    let opponentsHaveOneReward = all(function(elem) { return elem <= 1 },
      arrayReplace(goalCoeffsNumberByAgent, selfId, 0))
    if (isInitial(state) || opponentsHaveOneReward) {
      info("belief(): returning initial")
      return initialBelief
      // return retrieveBeliefOf(ofAgentId, initialBelief)
    }
    let prevState = getPreviousState(state)
    let lastAction = getLastAction(state)
    let prevBelief = belief(prevState)
    let result = updateBelief(prevBelief, prevState, lastAction)
    info("belief(state: " + stateToString(state) + "): returning " +
      beliefToString(result, goalCoeffsNumberByAgent))
    return result
  })



  /** belief here is over individual agent already */
  let updateBelief = function (belief, state, action) {
    /** for now not update to see performance changes */
    return belief
    let updateBeliefDiscrete = function (beliefVal, actingAgentID) {
      let updatedIndividualBelief = Infer({method: 'enumerate'}, function () {
        let beliefOverActingAgent = retrieveBeliefOver(actingAgentID, belief)
        let goalCoeffs = sampleBelief(beliefOverActingAgent)
        let cond = {
          representation: belief.representation,
          value: goalCoeffs
        }
        // let act = agents[agentID].act
        let predictedAction = sample(act(state, cond))
        factor(actionSimilarity(predictedAction, action))
        return goalCoeffs
      })
      /** update belief over that agent */
      let updatedBeliefValue = arrayReplace(belief.value, actingAgentID, updatedIndividualBelief)
      return {
        representation: 'discrete',
        value: updatedBeliefValue
      }
      /** for now lets assume we dont update belief */
      // return belief
    }

    /**
     * *belief* here is an array of parameters (of a dirichlet distribution)
     * The idea of an update is as follows: for each reward (physical or mental)
     * we compute the probability of agent taking *action* assuming they're solely
     * motivated by that reward. We then increment the distribution parameter corresponding
     * to that reward by the value of probability computed.
     */
    let updateBeliefDirichlet = function (beliefVal, actingAgentID) {
      info("updateBeliefDirichlet(): actingAgentID=" + actingAgentID + ", belief:")
      info(arrayToString(belief.value))
      /** compute the action under each possible reward, i.e. conditional action */
      let computeActionDist = function (index) {
        /** A 'conditional' call to act() in a sense that certain conditions are
         * placed on the execution of that call. In particular:
         * - value of trust computed based on *belief* is to be used for the purposes
         *    of computing utility of this agent in recursive act calls
         * - opponent is assumed to only care about reward number *index*
         */
        let cond = {
          representation: 'dirichlet',
          value: index
        }
        // let act = agents[agentID].act
        return act(state, cond) /** index is the condition */
      }
      let actingAgentsGoalCoeffIndexes =
        rangeArray(0, goalCoeffsNumberByAgent[actingAgentID]-1)
      let actionDists = map(computeActionDist, actingAgentsGoalCoeffIndexes)
      let actionProbs = map(function () {
        return Math.exp(actionDist.score(action))
      }, actionDists)
      display(actionProbs)
      // action probs might be very small; instead of adding them directly, we use them
      // as proportions and add a total of one
      let actionProbsSum = sum(actionProbs)
      let actionProbsNormalised = map(function (prob) {
        return prob / actionProbsSum
      }, actionProbs)
      assert(approxEqual(sum(actionProbsNormalised), 1),
        "Normalised action probabilities in updateBeliefDirichlet don't sum to 1")
      let updatedBeliefOverActingAgent =
        map2(add, beliefVal[actingAgentID], actionProbsNormalised)
      let updatedBelief = arrayReplace(beliefVal, actingAgentID, updatedBeliefOverActingAgent)
      info("updateBeliefDirichlet(): returning updatedBelief (value): ")
      info(arrayToString(updatedBelief))
      return {
        representation: 'dirichlet',
        value: updatedBelief
      }
    }

    info("updateBelief(): belief=" + toString(belief) + ", state=" +
    stateToString(state) + ", action=" + toString(action))
    let appropriateFn = {
      'discrete': updateBeliefDiscrete,
      'dirichlet': updateBeliefDirichlet
    }[belief.representation]

    let actingAgentID = turn(state)
    if (selfId == actingAgentID || actions(state).length == 1) {
      return belief
    }
    return appropriateFn(belief.value, actingAgentID)
    /** again, for now assume no update for simpl */
    // return belief
  }

  /** Utility function
   * Computes utility for agent *role* when *action* is performed (by whoever) in *state*.
   * Physical rewards are computed using physical reward structure
   * 1. If utility of this agent is being computed, mental rewards are computed
   * using mental rewards structure (dynamics functions) and goal coefficients are known
   * 2. Otherwise, there are two cases:
   * 2a. When no condition *cond* is passed then opponent's utility is computed with respect
   *     to this agent's belief and mental rewards are taken as real values this agent knows
   * 2b. When condition *cond* is passed, utility is computed wrt
   * This is achieved by computing income as a sum of endowment and transfer (can be negative)
   * and trust value using trust dynamics.
   */
  let actionUtility = dp.cache(function (state, action, ofAgentID, cond, mentalSnapshot) {
    info("actionUtility(state: " + stateToString(state) + ", action: " + action +
      ", role: " + ofAgentID + ", cond: " + condToString(cond) + ")")
    assertDefined(mentalSnapshot, "actionUtility: mentalSnapshot undefined")

    let rewards = actionRewards(state, action)[ofAgentID]
    let mentalRewardsDummy = repeat(numberOfRewards.mental, function () {
      return 0
    })
    if (ofAgentID == selfId) {
      // case 1
      info("actionUtility(): case 1: agent computes their own utility")
      return utilityFn(selfPhysicalGoalCoeffs, rewards)
    }
    if (cond !== undefined) {
      // case 2b
      info("actionUtility(): case 2b: agent computes other's utility on cond")
      // let condOfAgentID = extend(cond, {
      //   value: assertDefined(cond.value[ofAgentID],
      //     "goal coeffs for agent " + ofAgentID + " undefined")
      // })
      // return conditionalUtility(rewards.concat(mentalRewardsDummy), condOfAgentID)
      return conditionalUtility(rewards.concat(mentalRewardsDummy), cond, utilityFn)
    }
    info("actionUtility(): case 2a: agent computes other's utility")
    // TODO: which one should be used?
    // var belief = belief(state)
    let belief = retrieveBeliefOver(ofAgentID, belief(mentalSnapshot.state))

    // case 2a
    let expectations =
      goalCoeffsExpectation(belief, goalCoeffsNumberByAgent[ofAgentID]).slice(0, numberOfRewards.physical)
    return utilityFn(expectations, rewards)
  })

  let stateUtility = dp.cache(function (state, ofAgentID, cond, mentalSnapshot) {
    info("stateUtility(state: " + stateToString(state) + ", role: " + ofAgentID + ", cond: " + condToString(cond) + ")")
    assertDefined(mentalSnapshot,
      "actionUtility: mentalSnapshot undefined")

    let belief = belief(mentalSnapshot.state)
    let physicalRewards = stateRewards(state)[ofAgentID]
    if (ofAgentID == selfId) {
      info("stateUtility(): case 1")
      // case 1
      let mentalRewards = computeMentalRewards(state, ofAgentID, belief)
      info("stateUtility(): " + selfId + " computed rewards of " + ofAgentID +
        ": " + arrayToString(mentalRewards))
      return utilityFn(selfGoalCoeffs, physicalRewards.concat(mentalRewards))
    }

    if (cond !== undefined) {
      info("stateUtility(): case 2b")
      // case 2b
      // let condOfAgentID = extend(cond, {
      //   value: assertDefined(cond.value[ofAgentID],
      //     "goal coeffs for agent " + ofAgentID + " undefined")
      // })
      let mentalRewards = computeMentalRewards(state, ofAgentID, belief, mentalSnapshot)
      let result = conditionalUtility(physicalRewards.concat(mentalRewards), cond, utilityFn)
      return result
    }

    // case 2a
    info("stateUtility(): case 2a")
    // let belief = belief(mentalSnapshot.state)
    let mentalRewards = computeMentalRewards(state, ofAgentID, belief, mentalSnapshot)
    let ofAgentBelief = retrieveBeliefOver(ofAgentID, belief)
    let expectations = goalCoeffsExpectation(ofAgentBelief, goalCoeffsNumberByAgent[ofAgentID])
    let result = utilityFn(expectations, physicalRewards.concat(mentalRewards))
    info("stateUtility: returning " + result)
    return result
  })

  /** Compute action (distribution) at state
   * It either computes the action (distribution) for this agent (when no @cond
   * passed) or action (distribution) of opponent assuming @cond.
   *
   * @param state
   * @param cond (optional)
   * @return
    a distribution over actions available to the acting agent at @state
   */
  let act = dp.cache(function (state, condOpt) {
    info("act(state: " + stateToString(state) + ", cond: " + condToString(condOpt) + ")")
    let turn = turn(state)
    let thisAgentActs = turn === selfId
    explain("Agent " + selfId + " computes action at state " +
        stateToString(state), thisAgentActs)
    assert(thisAgentActs || condOpt !== undefined,
      "Input to act() doesn't meet precondition: condition must be passed")
    /** prepare mentalSnapshot for future utility computations) */
    let belief = belief(state)
    let mentalState = computeMentalState(state, belief)
    let mentalSnapshot = {
      values: mentalState,
      state
    }
    globalStore.indent = 0
    explain("Agent will explore the game tree up to depth " +
      selfMetaParams.lookAhead, thisAgentActs)
    let actionDist = Infer({method: 'enumerate'}, function () {
      let othersMetaParams = sampleMetaParamsEstimations(metaParamsEstimations)
      let allMetaParams = mergeMetaParams(othersMetaParams, selfMetaParams, selfId)
      let lookAhead = allMetaParams.lookAhead[turn]
      let actionDist = actRec(state, lookAhead, allMetaParams, condOpt, mentalSnapshot)
      return sample(actionDist)
    })
    let actionDistAsString = discreteDistributionToString(actionDist, actions(state))
    explain("Agent " + selfId + " computed the following action distribution:\n" +
        actionDistAsString, thisAgentActs)
    return actionDist
  })

  /**
   * Params:
   *   - *state* - a dictionary { turn, returns, investments }
   *   - *timeLeft* - an integer specifying time horizon of the decision making
   *   - *otherMetaParamsArr* - a dict consisting of arrays of opponents'
   *   meta-params (compulsory) and goal coefficients (optional)
   *   - *cond* - is an optional condition, used for belief update
   *   - *topLevel* - an extra optional argument for debugging purposes
   */
  let actRec = dp.cache(function (state, timeLeft, allMetaParams, cond, mentalSnapshot) {
    debug(spaces(globalStore.indent) + "actRec(): state=" + stateToString(state) +
      ", horizon: " + timeLeft + ")")
    let turn = turn(state)
    let alpha = assertDefined(allMetaParams.alpha[turn],
    "bad game specification: agent's " + selfId + " estimation " +
      "of agent's " + turn + " alpha undefined")
    let actionDist = Infer({method: 'enumerate'}, function () {
      let availableActions = assertDefined(actions(state),
        "bad game specification: actions at state " +
        stateToString(state) + " undefined")
      assertIsArray(availableActions, ANY_TYPE, -1,
        "bad game specification: actions at state " +
        stateToString(state) + " are not an array, found: " +
        toString(availableActions))
      // let action = timeLeft !== 9 ? uniformDraw(availableActions) : ["X",0]
      let action = uniformDraw(availableActions)
      debug("actRec(): computing expected utility for action "
        + action, globalStore.indent === 0)
      let eu = expectedUtility(state, action, turn, timeLeft, allMetaParams, cond, mentalSnapshot)
      factor(alpha * eu)
      return action
    })
    // let arr = arrayReplace(repeat(9, getUndefined), 0, 'X')
    // if (_.isEqual(state, arr)) {
    //   display("timeLeft: " + timeLeft)
    //   let actionDistAsString = discreteDistributionToString(actionDist, actions(state))
    //   display(actionDistAsString)
    // }
    globalStore.indent -= 2
    // let mostProbableAction = mostProbableValueInSupport(actionDist, actions(state))
    // info(spaces(globalStore.indent) + "Action at state " + stateToString(state) + " computed by " + selfId + ": " +
    //   mostProbableAction + ", with prob: " + Math.exp(actionDist.score(mostProbableAction)))
    return actionDist
  })

  /** Computes expected utility of agent with *role* upon *action* taken in *state*
   * with *timeLeft* left. *timeLeft* must be > 0!
   * Params:
   *   - *state* - a dictionary describing state, i.e. turn, returns and investments
   *   - *action* - a string identifying action taken in *state*
   *   - *role* - a string identifying the agent whose expected utility is to
   be computed
   *   - *timeLeft* - integer
   *   - *otherParams* -
   */
  let expectedUtility = dp.cache(
    function (state, action, ofAgentID, timeLeft, allMetaParams, cond, mentalSnapshot) {
    debug(spaces(globalStore.indent) + "expectedUtility() at state " +
      stateToString(state) + ", for action: " + action + ", computed by " +
      selfId + " for " + ofAgentID + ", with time horizon " + timeLeft +
      ", condition " + (cond === undefined ? "undefined" : cond.value))
    let u = actionUtility(state, action, ofAgentID, cond, mentalSnapshot)
    let nextTimeLeft = timeLeft - 1
    let discountFactor = assertDefined(allMetaParams.discountFactor[ofAgentID],
      "bad game specification: agent's " + selfId + " estimation " +
      "of agent's " + turn + " discountFactor undefined")
    globalStore.indent += 2
    let futureUtilityDist = Infer({method: 'enumerate'}, function () {
      let nextStateDist = transitionFn(state, action)
      assertHasType(nextStateDist, DIST_TYPE,
        "bad game specification: transitionFn must return a distribution, but " +
      "returned " + toString(nextStateDist) + " when called at state " +
        stateToString(state) + " with action " + action)
      let nextState = sample(nextStateDist)
      let nextStateUtility = stateUtility(nextState, ofAgentID, cond, mentalSnapshot)
      if (nextTimeLeft == 0) {
        return nextStateUtility
      }
      let nextTurn = turn(nextState)
      let nextActionTakerHorizon = min(allMetaParams.lookAhead[nextTurn], nextTimeLeft)
      let nextActionDist = actRec(nextState, nextActionTakerHorizon, allMetaParams, cond, mentalSnapshot)
      let nextAction = sample(nextActionDist)
      return nextStateUtility + expectedUtility(nextState, nextAction,
        ofAgentID, nextTimeLeft, allMetaParams, cond, mentalSnapshot)
    })
    let futureUtilExp = expectation(futureUtilityDist)
    let eu = u + discountFactor * futureUtilExp
    debug("exp util at state " + stateToString(state) +
      " for action " + action + ", with " + timeLeft + " steps left: "
      + eu,ofAgentID === selfId && cond === undefined && globalStore.indent === 2)
    return eu
  })

  return {
    params: selfParams,
    act,
    expectedUtility,
    belief,
    mentalRewards: function (state) {
      return computeMentalRewards(state)
    },
    mentalState: function (state, agentId, belief) {
      return computeMentalState(state, agentId, belief)
    }
  }
}

/** Helper functions */

let sampleMetaParamsEstimations = function (metaParamsEstimations) {
  info("sampleMetaParamsEstimations(" + metaParamsEstimations + ")")
  let sampleIfDefined = function(dist) {
    if (dist === undefined) return undefined
    assertHasType(dist, DIST_TYPE,
      "bad game specification: distribution expected in " +
      "metaParamsEstimations, found: " + toString(dist))
    return sample(dist)
  }
  return {
    alpha: map(sampleIfDefined, metaParamsEstimations.alpha),
    lookAhead: map(sampleIfDefined, metaParamsEstimations.lookAhead),
    discountFactor: map(sampleIfDefined, metaParamsEstimations.discountFactor)
  }
}

let mergeMetaParams = function(othersMetaParams, selfMetaParams, selfId) {
  info("mergeMetaParams(" + othersMetaParams + ", " + selfMetaParams + ")")
  return {
    alpha: arrayReplace(othersMetaParams.alpha, selfId, selfMetaParams.alpha),
    lookAhead: arrayReplace(othersMetaParams.lookAhead, selfId, selfMetaParams.lookAhead),
    discountFactor: arrayReplace(othersMetaParams.discountFactor, selfId, selfMetaParams.discountFactor),
  }
}