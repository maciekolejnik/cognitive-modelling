let LOGGING_LEVEL = 2
let EPSILON = Number.EPSILON * 10 // used for floating point comparisons

/** Auxiliary functions
*/

/** webppl prints arrays without [ and ] for some reason. fix that here
 */
let arrayToString = function(array) {
  return "[" + array + "]"
}

/**
 * This might be useful for debugging, returns string s in a box, i.e.
 * +---------------------+
 * |          s          |
 * +---------------------+
 * of appropriate width
 */
let stringInABox = function(s) {
  let dashes = "-".repeat(s.length + 4)
  let line = "+" + dashes + "+"
  let middle = "|  " + s + "  |"
  return line + "\n" + middle + "\n" + line
}

let multiply = function(a,b) {
  return a * b
}

let apply = function(f,x) {
  return f(x)
}

let add = function(a,b) {
  return a + b
}

let approxEqual = function(a,b) {
  return Math.abs(a - b) < EPSILON
}

/**
 * if obj is defined, return it, otherwise, return fallback
 */
let ifDefinedElse = function(obj, fallback) {
  return obj || fallback
}

let assertDefined = function(x) {
  console.assert(x !== undefined)
  return x
}

// returns an array [start, start+1,...,end]
let rangeArray  = function(start,end,stepOptional) {
  console.assert(start !== undefined && end !== undefined,
    "Invalid input to rangeArray")
  let step = ifDefinedElse(stepOptional, 1)
  return (start > end) ? [] : [start].concat(rangeArray(start+step,end, step))
}

/**
 * returns the most probable value in a support of a given distribution.
 * support must be passed separately as it is not accesible from the distribution object
 */
let mostProbableValueInSupport = function(dist, support) {
  let mostProbableValueInSupportHelper = function(dist, support, maxProbSoFar, valueWithMaxProb) {
    if (support.length == 0) return valueWithMaxProb
    let thisValue = support[0]
    let thisValueProb = Math.exp(dist.score(thisValue))
    let supportTail = support.slice(1)
    if (thisValueProb > maxProbSoFar) {
      return mostProbableValueInSupportHelper(dist, supportTail, thisValueProb, thisValue)
    }
    return mostProbableValueInSupportHelper(dist, supportTail, maxProbSoFar, valueWithMaxProb)
  }
  return mostProbableValueInSupportHelper(dist, support, 0, undefined)
}

let sampleMetaParamsEstimations = function(metaParamsEstimations) {
  return {
    alpha: sample(metaParamsEstimations.alpha),
    lookAhead: sample(metaParamsEstimations.lookAhead),
    discountFactor: sample(metaParamsEstimations.discountFactor)
  }
}

/** Logging 
  * levels 0-5
  * 0 is no logging
  * 1 only log critical (critical)
  * 4 is log all but info (critical, debug)
  * 5 is log everything (critical, debug, info)
*/
let getLogger = function(level) {
  let info = function(message) {
    if (level >= 5) {
      display(message)
    }
  }

  let debug = function(message) {
    if (level >= 4) {
      display(message)
    }
  }

  let explain = function(message) {
    if (level >= 3) {
      display(message)
    }
  }

  let critical = function(message) {
    if (level >= 1) {
      display(message)
    }
  }

  return {info, debug, explain, critical}
}
let logger = getLogger(LOGGING_LEVEL)
let debug = logger.debug
let info = logger.info
let explain = logger.explain
let critical = logger.critical


let getGameAPI = function(representation, numberOfRewards, gameSpecificAPI) {
  let stateToString = gameSpecificAPI.stateToString

  let goalCoeffExpectation = function(belief, index) {
    // index must be an integer
    let goalCoeffExpectationDiscrete = function() {
      return expectation(marginalize(belief, '' + index))
    }

    // belief is an array of parameters
    let goalCoeffExpectationDirichlet = function() {
      return belief[index] / sum(belief)
    }

    let goalCoeffExpectationFunctions = {
      discrete: goalCoeffExpectationDiscrete,
      dirichlet: goalCoeffExpectationDirichlet
    }

    console.assert(index >= 0 && index < numberOfRewards,
      "Invalid index in goalCoeffExpectation: " + index)
    let appropriateFunction = goalCoeffExpectationFunctions[representation]
    return appropriateFunction()
  }

  /** Conditional utility
   * When updating their beliefs, agents need to compute expected
   * utility of their opponent under certain condition. What the
   * condition is depends on the way beliefs are represented.
   * - For discrete distribution representation of belief, condition
   *   is as in bayesian update, i.e. an array of goal coefficients,
   *   accessed at 'cond.goalCoeffs'
   * - For Dirichlet representation of belief, an index (of a reward)
   *   is the condition, accessed with 'cond.index'
   **/
  let conditionalUtility = function(rewards, cond) {
    // in this case condition is an array of goal coefficients
    let conditionalUtilityDiscrete = function(rewards, cond) {
      info("conditionalUtilityDiscrete(rewards: " + arrayToString(rewards)
        + ", " + condToString(cond) + ")")
      return utilityFn(cond.goalCoeffs, rewards)
    }

    // in this case, condition is an index of reward to use
    let conditionalUtilityDirichlet = function(rewards, cond) {
      info("conditionalUtilityDirichlet(rewards: " + arrayToString(rewards)
        + ", " + condToString(cond) + ")")
      return rewards[cond.index]
    }

    let conditionalUtilityFunctions = {
      discrete: conditionalUtilityDiscrete,
      dirichlet: conditionalUtilityDirichlet
    }

    let appropriateFunction = conditionalUtilityFunctions[representation]
    return appropriateFunction(rewards, cond)
  }

  /** returns array of expectations of goal coefficients */
  let goalCoeffsExpectation = function(belief) {
    return map(function(index) { return goalCoeffExpectation(belief, index)},
      rangeArray(0, numberOfRewards-1))
  }

  let beliefAPI = {
    goalCoeffExpectation,
    goalCoeffsExpectation,
    conditionalUtility
  }

  /** Printing */

  let condToString = function(cond) {
    var condToStringDiscrete = function() {
      let coeffs = "coeffs: " + arrayToString(cond.goalCoeffs)
      // let mentalSnapshot = "mentalSnapshot: " + mentalSnapshotToString(cond.mentalSnapshot)
      return "{" + coeffs + "}"
    }

    var condToStringDirichlet = function() {
      let index = "index: " + cond.index
      // let mentalSnapshot = "mentalSnapshot: " + mentalSnapshotToString(cond.mentalSnapshot)
      return "{" + index + "}"
    }

    var condToStringFunctions = {
      discrete: condToStringDiscrete,
      dirichlet: condToStringDirichlet
    }

    if (cond === undefined) return "undefined"
    var appropriateFunction = condToStringFunctions[representation]
    return appropriateFunction(cond)
  }

  let beliefToString = function(belief) {
    let beliefToStringDiscrete = function() {
      return "expectations: " + goalCoeffsExpectation(belief)
    }

    let beliefToStringDirichlet = function() {
      return arrayToString(belief)
    }

    let beliefToStringFunctions = {
      discrete: beliefToStringDiscrete,
      dirichlet: beliefToStringDirichlet
    }

    let appropriateFunction = beliefToStringFunctions[representation]
    return appropriateFunction()
  }

  let mentalSnapshotToString = function(mentalSnapshot) {
    let state = stateToString(mentalSnapshot.state)
    let values = mentalSnapshot.values
    return "{state: " + state + ", values: " + values + "}"
  }

  let printAPI = {
    stateToString,
    condToString,
    beliefToString,
    mentalSnapshotToString
  }

  return {
    belief: beliefAPI,
    print: printAPI,
    gameSpecific: gameSpecificAPI
  }
}


/** makeCSMG (cognitive stochastic multiplayer game)
 * @param gameSetup
 *  an object of the form
 *  {
    actions,
    transition,
    initialState,
    API,
    getPhysicalRewards,
    getMentalStateDynamics,
    utilityFn,
    params
    }
 * Given a game configuration, this generic function creates a
 * cognitive model.
 */
let makeCSMG = function(gameSetup) {

  let params = gameSetup.params
  let beliefRepresentation = params.beliefRepresentation
  let numberOfRewards = params.numberOfRewards

  let actions = gameSetup.actions
  let transitionFn = gameSetup.transitionFn
  let initialState = gameSetup.initialState

  /** Extract game-specific API calls for easy access */

  let API = getGameAPI(beliefRepresentation, numberOfRewards, gameSetup.API)
  let beliefAPI = API.belief

  let gameSpecificAPI = API.gameSpecific
  let getPreviousState = gameSpecificAPI.getPreviousState
  let getLastAction = gameSpecificAPI.getLastAction
  let isInitial = gameSpecificAPI.isInitial
  let other = gameSpecificAPI.other

  /** Reward structures */
  let physicalRewardStructure = gameSetup.physicalRewardStructure
  let getMentalStateDynamics = gameSetup.getMentalStateDynamics

  /** Reward Utilities */
  let rewardUtilityFunctions = gameSetup.rewardUtilityFunctions

  /** Physical rewards structure
   *  This simply delegates to the physicalRewards object as given
   *  in gameSetup
   */
  let getPhysicalRewardStructure = function() {
    return physicalRewardStructure
  }

  /** Mental reward structure uses mental state dynamics provided in
   * @gameSetup to enable computation of mental rewards in any state.
   * Primarily, this is used by an agent to estimate mental state of
   * their opponent.
   * However, it is also used by an agent to estimate their own mental
   * state at some future point, thereby not requiring to compute belief
   * of that agent.
   *
   * @param initialEstimations (@type array of distributions)
   *  initial mental state estimations of this agent
   * @param agentID
   *  some form of identifier (which is game specific) of an agent
   *  identifies the agent for which this reward structure is provided
   */
  let getMentalRewardStructure = function(initialEstimations, agentID) {
    /**
     * @type array of update functions (each of @type (value, agent, state, action) -> newValue)
     */
    let mentalStateDynamics = getMentalStateDynamics(beliefAPI)
    let estimationHeuristicsArr = mentalStateDynamics.estimationHeuristicArr
    let mentalStateArr = mentalStateDynamics.mentalStateArr
    console.assert(mentalStateArr.length == initialEstimations.length,
      "getMentalRewardStructure: Dimensions mismatch between "
      + "mentalStateArr (" + mentalStateArr.length + ") and "
      + "initialEstimations (" + initialEstimations.length + ")")

    /** Compute mental rewards of one of the agents from the perspective of @agentID.
     * There are two cases:
     * - if @mentalSnapshot is missing, @agentID is computing its own mental rewards
     *   as part of selecting its best action (expected utility)
     * - if @mentalSnapshot is passed, @agentID is computing its opponent's mental
     *   rewards, either as part of selecting an action or updating belief
     * @param state
     * @param mentalSnapshot (optional)
     *  a reference point from which to use mental state dynamics
     *  it's an object { state, values }
     *  (it snapshots mental state values @estimations at @state)
     *  if omitted, initial state with initial estimations serves as snapshot
     */
    let computeMentalRewards = function(state, mentalSnapshot) {
      if (mentalSnapshot !== undefined) {
        return computeMentalRewardRecFromSnapshot(state, mentalSnapshot)
      }
      let estimations = computeMentalStateEstimations(state)
      let estimationExpectation = function(estimation) {
        return expectation(estimation)
      }
      let result = map(estimationExpectation, estimations)
      // if (state.investments.length == 1 && state.returns.length < 1) display(arrayToString(result))
      return result
    }

    let computeMentalState = function(state, belief) {
      let f = function(mentalStateComputeFn) {
        return mentalStateComputeFn(state, belief)
      }
      return map(f, mentalStateArr)
    }

    let computeMentalRewardRecFromSnapshot = dp.cache(function(state, mentalSnapshot) {
      console.assert(mentalSnapshot !== undefined,
        "mentalRewardRecFromSnapshot: Mental snapshot undefined!")
      let snapState = mentalSnapshot.state
      let snapValues = mentalSnapshot.values
      if (_.isEqual(state, snapState)) {
        return snapValues
      }
      let prevState = getPreviousState(state)
      let lastAction = getLastAction(state)
      let prevValues = computeMentalRewardRecFromSnapshot(prevState, mentalSnapshot)
      let updateValue = function(updateFn, prevValue) {
        return updateFn(prevValue, agentID, prevState, lastAction)
      }
      let curValues = map2(updateValue, estimationHeuristicsArr, prevValues)
      return curValues
    })

    let computeMentalStateEstimations = dp.cache(function(state) {
      if (isInitial(state)) return initialEstimations
      let prevState = getPreviousState(state)
      let lastAction = getLastAction(state)
      let prevEstimations = computeMentalStateEstimations(prevState)
      let updateEstimation = function(updateFn, prevEstimation) {
        return Infer({method: 'enumerate'}, function() {
          let prevValue = sample(prevEstimation)
          return updateFn(prevValue, other(agentID), prevState, lastAction)
        })
      }
      let curEstimations = map2(updateEstimation, estimationHeuristicsArr, prevEstimations)
      return curEstimations
    })

    return {
      computeMentalRewards,
      computeMentalState,
      quantity: initialEstimations.length
    }
  }

  let utilityFn = function(goalCoeffs, rewards) {
    console.assert(goalCoeffs.length == rewards.length,
      "Dimension of goal coefficients: " + goalCoeffs.length +
       " doesn't match rewards: " + rewards.length)
    let rewardUtilities = map2(apply, rewardUtilityFunctions, rewards)
    let result = sum(map2(multiply, goalCoeffs, rewardUtilities))
    info("utility computed: " + result)
    return result
  }

  return {
    actions,
    transitionFn,
    initialState,
    API,
    getPhysicalRewardStructure,
    getMentalRewardStructure,
    utilityFn,
    params
  }
}

/** Cognitive trust aware agent

 * As described in the paper, an agent is characterised by
 * 1. an array of coefficients
 *    *goalCoeffs* = [a1,a2,...,an]
 *    (we require a1 + a2 + ... + an = 1)
 * 2. meta-parameters
 *    *metaParams* = {
 *      alpha: [0,inf),
 *      lookAhead: [0, inf) (integer),
 *      discountFactor: (0,1]
 *    }
 * The above dictionaries are stored in a single dict *selfParams*
 * Moreover, each agent has a role (*selfId*), which is 'investor' or 'investee'
 * Next, each agent has an initial state, consisting of
 * 3. initial belief about opponent's mental characteristics. 
 *   Currently assumed to be Dirichlet distribution and represented by 
 *   its set of parameters. However, implementation should be representation-agnostic:
 *   we provide certain operations on belief:
 *   - updateBelief
 * 4. initial estimations of mental rewards of agent's opponent.
 *    At the minimum, this contains estimation of trust.
 *    It is an array.
 *    Eg [<trustEstimation:distribution>]
 * 5. estimation of opponent's mental parameters
 *    metaParamsEstimations = distribution over values 
      {
        alpha: <discrete dist>,
        lookAhead: <discrete dist>,
        discountFactor: <discrete dist>
      }
   Hence, *initialState* takes a form of a following dictionary {
     belief: <3>,
     mentalEstimations: <4>,
     metaParamsEstimations: <5>
   }
 *   
*/
// TODO: could be nice here if gamespecific api was allowed to assert params
//  passed are valid (eg selfid needs to be investor of investee)
let makeAgent = function(selfParams, selfId, initialState, game) {
  debug("makeAgent(selfParams: ")
  debug(selfParams)
  debug("selfId: ")
  debug(selfId)
  debug("initialState: ")
  debug(initialState)
  debug("game: ")
  debug(game)
  /** Extract some functions from the game API for easy access */
  let gameSpecificApi = game.API.gameSpecific
  console.assert(gameSpecificApi !== undefined, "gameSpecificAPI undefined")
  let isInitial = gameSpecificApi.isInitial
  let getPreviousState = gameSpecificApi.getPreviousState
  let getLastAction = gameSpecificApi.getLastAction
  let stateToString = gameSpecificApi.stateToString
  let turn = gameSpecificApi.turn

  let beliefAPI = game.API.belief
  console.assert(beliefAPI !== undefined, "beliefAPI undefined")
  // let goalCoeffExpectation = beliefAPI.goalCoeffExpectation
  let goalCoeffsExpectation = beliefAPI.goalCoeffsExpectation
  let conditionalUtility = beliefAPI.conditionalUtility

  let printAPI = game.API.print
  console.assert(printAPI !== undefined, "printAPI undefined")
  let condToString = printAPI.condToString
  // let mentalSnapshotToString = printAPI.mentalSnapshotToString
  let beliefToString = printAPI.beliefToString

  /* Extract some fields for easier access */
  let transitionFn = game.transitionFn
  let actions = game.actions
  let utilityFn = game.utilityFn

  let gameParams = game.params
  let beliefRepresentation = gameParams.beliefRepresentation

  console.assert(selfId !== undefined)
  /* Meta-parameters */
  console.assert(selfParams !== undefined, "makeAgent: selfParams not passed")
  let selfMetaParams = selfParams.metaParams
  console.assert(selfMetaParams !== undefined, "makeAgent: metaParams missing")
  let selfLookAhead = selfMetaParams.lookAhead
  console.assert(selfLookAhead !== undefined, "makeAgent: lookAhead missing")
  let selfAlpha = selfMetaParams.alpha
  console.assert(selfAlpha !== undefined, "makeAgent: alpha missing")
  let selfDiscountFactor = selfMetaParams.discountFactor
  console.assert(selfDiscountFactor !== undefined, "makeAgent: discountFactor missing")

  /* Initial state */
  console.assert(initialState !== undefined, "makeAgent: initialState missing")
  let initialBelief = initialState.belief
  console.assert(initialBelief !== undefined, "makeAgent: initialBelief missing")
  let initialMentalEstimations = initialState.mentalEstimations
  console.assert(initialMentalEstimations !== undefined, "makeAgent: initialMentalEstimations missing")
  let metaParamsEstimations = initialState.metaParamsEstimations
  console.assert(metaParamsEstimations !== undefined, "makeAgent: metaParamsEstimations missing")

  // let beliefAPI = getBeliefAPI()
  // let goalCoeffExpectation = beliefAPI.goalCoeffExpectation
  // let conditionalUtility = beliefAPI.conditionalUtility
  // let beliefToString = beliefAPI.beliefToString
  // let condToString = beliefAPI.condToString

  /* Reward structures */
  let getPhysicalRewardStructure = game.getPhysicalRewardStructure
  let physicalRewardStructure = getPhysicalRewardStructure()
  console.assert(physicalRewardStructure !== undefined, 'physicalRewardStructure undefined')
  let stateRewards = physicalRewardStructure.stateRewards
  let actionRewards = physicalRewardStructure.actionRewards
  let numberOfPhysicalRewards = physicalRewardStructure.quantity

  let getMentalRewardStructure = game.getMentalRewardStructure
  let mentalRewardStructure = getMentalRewardStructure(initialMentalEstimations, selfId)
  console.assert(mentalRewardStructure !== undefined, 'mentalRewardStructure undefined')
  let computeMentalRewards = mentalRewardStructure.computeMentalRewards
  let computeMentalState = mentalRewardStructure.computeMentalState
  let numberOfMentalRewards = mentalRewardStructure.quantity
  let numberOfRewards = numberOfMentalRewards + numberOfPhysicalRewards

  /* Goal coefficients of this agent */
  let selfGoalCoeffs = selfParams.goalCoeffs
  let selfPhysicalGoalCoeffs = selfGoalCoeffs.slice(0, numberOfPhysicalRewards)
  let selfMentalGoalCoeffs = selfGoalCoeffs.slice(numberOfPhysicalRewards)

  // console.assert(selfGoalCoeffs.length == initialMentalEstimations.length + ??)


  /*********************
   * Belief operations *
   *********************
   Abstractly, belief is a continuous probability distribution.
   Concretely, two finite representations are proposed:
   - a discrete distribution that approximates a continuous distribution
   - a set of parameters of dirichlet distribution; here we make an
   assumption that belief follows dirichlet dist
   We would like to be able to change between those, and possibly more,
   representations easily, while exposing a uniform interface for the
   rest of the code that uses belief.
   We therefore include the following methods in the 'interface' of belief
   - updateBelief(belief, observation)
   - sampleBelief(belief)
   - beliefToString(belief)
   returns a vector of goal coefficients
   */

  /** Belief
   *  Computes belief of this agent in *state*. Proceeds recursively.
   *  To avoid recomputation, this function is cached.
   */
  let belief = dp.cache(function(state) {
    info("belief(state: " + stateToString(state) + ") for " + selfId)
    if (isInitial(state)) {
      return initialBelief
    }
    let prevState = getPreviousState(state)
    let lastAction = getLastAction(state)
    let prevBelief = belief(prevState)
    let result = updateBelief(prevBelief, prevState, lastAction)
    info("belief(state: " + stateToString(state) + "): returning " + beliefToString(result))
    return result
  })

  // this one is heavy
  let updateBelief = function(belief, state, action) {
    var updateBeliefDiscrete = function() {
      var dist = Infer({method: 'forward', samples: 100}, function() {
        var metaParams = sample(metaParamsEstimations)
        var goalCoeffs = sampleBelief(belief)
        var otherParams = {
          metaParams,
          goalCoeffs
        }
        var predictedAction = sample(act(state, otherParams))
        condition(predictedAction == action)
        return params
      })
      return dist
    }

    /**
    * *belief* here is an array of parameters (of a dirichlet distribution)
    * The idea of an update is as follows: for each reward (physical or mental)
    * we compute the probability of agent taking *action* assuming they're solely
    * motivated by that reward. We then increment the distribution parameter corresponding
    * to that reward by the value of probability computed.
    */
    let updateBeliefDirichlet = function() {
      // compute the action under each possible reward
      let computeActionDist = function(index) {
        /* A 'conditional' call to act() in a sense that certain conditions are
         * placed on the execution of that call. In particular:
         * - value of trust computed based on *belief* is to be used for the purposes
         *    of computing utility of this agent in recursive act calls
         * - opponent is assumed to only care about reward number *index*
        */
        let cond = { index }
        return act(state, cond) // index is the condition
      }
      let actionDists = map(computeActionDist, rangeArray(0, belief.length-1))
      let getProbOfAction = function(actionDist) {
        return Math.exp(actionDist.score(action))
      }
      var actionProbs = map(getProbOfAction, actionDists)
      // action probs might be very small; instead of adding them directly, we use them
      // as proportions and add a total of one
      let actionProbsSum = sum(actionProbs)
      let actionProbsNormalised = map(function(prob) {return prob/actionProbsSum}, actionProbs)
      console.assert(approxEqual(sum(actionProbsNormalised), 1),
        "Normalised action probabilities in updateBeliefDirichlet don't sum to 1")
      let result = map2(add, belief, actionProbsNormalised)
      return result
    }

    let updateBeliefFunctions = {
      discrete: updateBeliefDiscrete,
      dirichlet: updateBeliefDirichlet
    }

    // display("updateBelief(belief: " + beliefToString(belief) + ", state: " +
    //   stateToString(state) + ", action: " + action + ")")
    if (selfId == state.turn || actions(state).length == 1) {
      return belief
    }
    var appropriateFunction = updateBeliefFunctions[beliefRepresentation]
    let result = appropriateFunction()
    // display("updateBelief(): return " + beliefToString(result))
    return result
  }

  let sampleBelief = function(belief) {
    // belief is a distribution over unit vectors
    let sampleBeliefDiscrete = function(belief) {
      return sample(belief)
    }

    // belief is an array of parameters
    let sampleBeliefDirichlet = function(belief) {
      return Dirichlet({alpha: Vector(belief)}).sample()
      // return sample(Dirichlet({alpha: Vector(belief)}))
    }

    let sampleBeliefFunctions = {
      discrete: sampleBeliefDiscrete,
      dirichlet: sampleBeliefDirichlet
    }

    let appropriateFunction = sampleBeliefFunctions[beliefRepresentation]
    return appropriateFunction(belief)
  }

  /** Utility function 
    * Computes utility for agent *role* when *action* is performed (by whoever) in *state*.
    * Physical rewards are computed using physical reward structure
    * 1. If utility of this agent is being computed, mental rewards are computed 
    * using mental rewards structure (dynamics functions) and goal coefficients are known
    * 2. Otherwise, there are two cases:
    * 2a. When no condition *cond* is passed then opponent's utility is computed with respect 
    *     to this agent's belief and mental rewards are taken as real values this agent knows
    * 2b. When condition *cond* is passed, utility is computed wrt 
    * This is achieved by computing income as a sum of endowment and transfer (can be negative)
    * and trust value using trust dynamics.
  */
  let actionUtility = dp.cache(function(state, action, role, cond, mentalSnapshot) {
    info("actionUtility(state: " + stateToString(state) + ", action: " + action +
        ", role: " + role + ", cond: " + condToString(cond) + ")")
    console.assert(mentalSnapshot !== undefined,
      "actionUtility: mentalSnapshot undefined")

    let rewards = actionRewards(state, action)[role]
    let mentalRewardsDummy = repeat(numberOfMentalRewards, function() {return 0})

    if (role == selfId) {
      // case 1
      return utilityFn(selfPhysicalGoalCoeffs, rewards)
    }

    if (cond !== undefined) {
      // case 2b
      return conditionalUtility(rewards.concat(mentalRewardsDummy), cond)
    }

    // TODO: which one should be used?
    // var belief = belief(state)
    let belief = belief(mentalSnapshot.state)

    // case 2a
    let expectations = goalCoeffsExpectation(belief).slice(0, numberOfPhysicalRewards)
    return utilityFn(expectations, rewards)
  })

  let stateUtility = dp.cache(function(state, role, cond, mentalSnapshot) {
    info("stateUtility(state: " + stateToString(state) + ", role: " + role + ", cond: " + condToString(cond) + ")")
    console.assert(mentalSnapshot !== undefined,
      "actionUtility: mentalSnapshot undefined")

    let physicalRewards = stateRewards(state)[role]
    if (role == selfId) {
      let mentalRewards = computeMentalRewards(state)
      return utilityFn(selfGoalCoeffs, physicalRewards.concat(mentalRewards))
    }

    if (cond !== undefined) {
      // case 2b
      let mentalRewards = computeMentalRewards(state, mentalSnapshot)
      let result = conditionalUtility(physicalRewards.concat(mentalRewards), cond)
      return result
    }

    // case 2a
    let belief = belief(mentalSnapshot.state)
    let mentalRewards = computeMentalRewards(state, mentalSnapshot)
    let expectations = goalCoeffsExpectation(belief)
    let result = utilityFn(expectations, physicalRewards.concat(mentalRewards))
    return result
  })

  /** This represents how this agent computes their action */
  let actSelf = dp.cache(function(state) {
    let mentalSnapshot = {
      values: computeMentalState(state, belief(state)),
      state
    }
    let result = Infer({method: 'enumerate'}, function() {
      let otherMetaParams = sampleMetaParamsEstimations(metaParamsEstimations)
      let actionDist = actSelfRec()
      let actionDist = actRec(state, lookAhead, otherMetaParams, cond, mentalSnapshot, true && cond == undefined)
      return sample(actionDist)
    })
  })

  /** Compute action (distribution) at state
   * It either computes the action (distribution) for this agent (when no @cond
   * passed) or action (distribution) of opponent assuming @cond.
   *
   * @param state
   * @param cond (optional)
   * @return
      a distribution over actions available to the acting agent at @state
  */
  let act = dp.cache(function(state, cond) {
    info("act(state: " + stateToString(state) + ", cond: " + condToString(cond) + ")")
    // var start = _.now()
    explain("Action at state " + stateToString(state) + " being computed by " + selfId)
    let thisAgentActs = turn(state) == selfId
    console.assert(thisAgentActs || cond !== undefined, 
      "Input to act() doesn't meet precondition: condition must be passed")
    // prepare mentalSnapshot for future utility computations)
    let mentalSnapshot = {
      values: computeMentalState(state, belief(state)),
      state
    }
    let result = Infer({method: 'enumerate'}, function() {
      let otherMetaParams = sampleMetaParamsEstimations(metaParamsEstimations)
      let lookAhead = thisAgentActs ? selfLookAhead : otherMetaParams.lookAhead
      let actionDist = actRec(state, lookAhead, otherMetaParams, cond, mentalSnapshot, true && cond == undefined)
      return sample(actionDist)
    })
    let mostProbableAction = mostProbableValueInSupport(result, actions(state))
    explain("Action at state " + stateToString(state) + " computed by " + selfId + ": " + mostProbableAction)
    // debug("act(state: " + stateToString(state) + ", cond: " + condToString(cond) + ")"
    //   + " finished in " + (_.now() - start) + " msec.")
    return result
  })

  /**
  * Params:
  *   - *state* - a dictionary { turn, returns, investments }
  *   - *timeLeft* - an integer specifying time horizon of the decision making
  *   - *otherMetaParams* - a dict consisting of opponents' meta-params (compulsory)
  *       and goal coefficients (optional)
  *   - *cond* - is an optional condition, used for belief update
  *   - *topLevel* - an extra optional argument for debugging purposes 
  */
  let actRec = dp.cache(function(state, timeLeft, otherMetaParams, cond, mentalSnapshot, topLevel) {
    info("actRec(" + stateToString(state) + ", " + timeLeft + ", " + otherMetaParams + ")")
    let alpha = (turn(state) == selfId) ? selfAlpha : otherMetaParams.alpha
    let actionDist = Infer({method: 'enumerate'}, function() {
      let action = uniformDraw(actions(state))
      let eu = expectedUtility(state, action, turn(state), timeLeft, otherMetaParams, cond, mentalSnapshot, topLevel)
      // if (_.isEqual(state.investments, [1]) && state.returns.length == 0 && selfId == 'investor' && turn(state) == 'investee') {
      //   display(action + ": " + eu)
      // }
      factor(alpha * eu)
      return action
    })
    let mostProbableAction = mostProbableValueInSupport(actionDist, actions(state))
    explain("Action at state " + stateToString(state) + " computed by " + selfId + ": " + mostProbableAction)
    return actionDist
  })

  /** Computes expected utility of agent with *role* upon *action* taken in *state*
    * with *timeLeft* left
    * Params:
    *   - *state* - a dictionary describing state, i.e. turn, returns and investments
    *   - *action* - a string identifying action taken in *state*
    *   - *role* - a string identifying the agent whose expected utility is to 
            be computed
    *   - *timeLeft* - integer 
    *   - *otherParams* - 
  */
  let expectedUtility = dp.cache(function(state, action, role, timeLeft, otherMetaParams, cond, mentalSnapshot, topLevel) {
    info("expectedUtility(" + stateToString(state) + ", " + action + ", " + role + ", " + timeLeft + ")")
    let start = _.now()
    let u = actionUtility(state, action, role, cond, mentalSnapshot)
    let nextTimeLeft = timeLeft - 1
    let discountFactor = (role == selfId) ? selfDiscountFactor : otherMetaParams.discountFactor
    let eu = u + discountFactor * expectation(Infer({method: 'enumerate'}, function() {
      let nextState = transitionFn(state, action)
      let nextStateUtility = stateUtility(nextState, role, cond, mentalSnapshot)
      if (nextTimeLeft == 0) {
        return nextStateUtility
      }
      let nextActionDist = actRec(nextState, nextTimeLeft, otherMetaParams, cond, mentalSnapshot)
      let nextAction = sample(nextActionDist)
      return nextStateUtility +
        expectedUtility(nextState, nextAction, role, nextTimeLeft, otherMetaParams, cond, mentalSnapshot)
    }))
    if (topLevel) {
      let eeu = eu + stateUtility(state, role, cond, mentalSnapshot)
      explain("[" + action + "," + timeLeft + "," + role + "," + state.investments +
         "," + state.returns + "]. Took " + (_.now() - start) + ". EU=" + eeu)
    }
    return eu
  })

  return {
    params: selfParams,
    act,
    expectedUtility,
    belief,
    mentalRewards: function(state) {return computeMentalRewards(state)}
  }
}