/** Some integration test are defined in this file.
 * They're specified in an array below; each element is a complete
 * specification of a cognitive game. They are roughly ordered in
 * increasing order wrt to complexity.
 *
 * From the commandline, run
 $ webppl test.wppl --require ../webppl-cognitive-agents
 */
let tests = function() {

  let testsArray = [
    /** FIRST TEST (one player trivial) */
    function() {
      let makeGame = function () {
        let actions = getConstantFn(["bottom"])

        let transitionFn = function (state, action) {
          return Delta({v: state + 1})
        }

        let API = function () {
          let getPreviousState = function (state) {
            return state - 1
          }
          let getLastAction = function (state) {
            assert(!isInitial(state));
            return 'bottom'
          }
          let isInitial = function (state) {
            return state == 0
          }
          let turn = getConstantFn(0)
          /** since there's only one player */
          let other = function (player) {
            return (player + 1) % 2
          }
          let stateToString = identity
          let API = {
            getPreviousState,
            getLastAction,
            isInitial,
            turn,
            other,
            stateToString
          }
          return API
        }()

        let physicalRewardStructure = function () {
          let stateRewards = function (state) {
            return [[1]]
          }
          let actionRewards = function (state, action) {
            return [[0]]
          }
          return {
            actionRewards,
            stateRewards,
            quantity: 1
          }
        }()

        /** No mental rewards for simplicity */
        let mentalStateDynamics = function () {
          return {
            estimationHeuristicArr: [],
            mentalStateArr: [],
            mentalUtilities: [[]]
          }
        }()

        let initialState = 0

        let params = {
          beliefRepresentation: 'dirichlet',
          numberOfAgents: 1,
          numberOfRewards: {
            physical: 1,
            mental: 0
          }
        }

        let rewardUtilityFunctions = [identity]

        return {
          actions,
          transitionFn,
          initialState,
          physicalRewardStructure,
          mentalStateDynamics,
          rewardUtilityFunctions,
          API,
          params
        }
      }

      /** Define a basic scenario */
      let scenarios = function() {
        let agent = {
          params: {
            goalCoeffs: [1],
            metaParams: {
              alpha: 100,
              discountFactor: 0.8,
              lookAhead: 2
            }
          },
          initialState: {
            belief: [undefined],
            mentalEstimations: [undefined],
            metaParamsEstimations: {
              alpha: [undefined],
              lookAhead: [undefined],
              discountFactor: [undefined]
            }
          }
        }

        let result = [
          {
            name: 'basic',
            agents: [ agent ],
            options: {
              horizon: 10,
              beliefRepresentation: 'dirichlet'
            }
          },
          {
            name: 'basic with discrete belief',
            agents: [ agent ],
            options: {
              horizon: 10,
              beliefRepresentation: 'discrete'
            }
          }
        ]

        return result
      }()

      let agents = ['bobby']

      let description = 'Trivial test with single state, single agent ' +
        'and single action, but repeated.'

      let validateOutput = function(trajectories) {
        assertIsArray(trajectories, OBJECT_TYPE, 2,
          "FAIL: trajectories array not as expected")
        let trajectory = trajectories[0]
        map(function(trajectory) {
          assertIsArray(trajectory, OBJECT_TYPE, 10,
            "FAIL: trajectory: " + trajectory + "not as expected")
          mapIndexed(function (i, elem) {
            assertEqual(elem, [i, 'bottom'])
          }, trajectory)
        }, trajectories)
      }

      return {
        makeGame,
        scenarios,
        agents,
        description,
        validateOutput
      }
    }(),

    /** SECOND TEST (one player simple) */
    function() {
      let makeGame = function () {
        let actions = function (state) { return ['bad', 'good'] }

        let transitionFn = function (state, action) {
          let nextState = [action].concat(state)
          return Delta({v: nextState})
        }

        let API = function () {
          let getPreviousState = function (state) {
            return state.slice(1)
          }
          let getLastAction = function (state) {
            assert(!isInitial(state));
            return state[0]
          }
          let isInitial = function (state) {
            return state.length == 0
          }
          let turn = getConstantFn(0)
          /** since there's only one player */
          let other = function (player) {
            return (player + 1) % 2
          }
          let stateToString = arrayToString
          let API = {
            getPreviousState,
            getLastAction,
            isInitial,
            turn,
            other,
            stateToString
          }
          return API
        }()

        let physicalRewardStructure = function () {
          let stateRewards = function (state) {
            return [[0]]
          }
          let actionRewards = function (state, action) {
            let reward = {
              'bad': -1,
              'good': 1
            }[action]
            return [[reward]]
          }
          return {
            actionRewards,
            stateRewards,
            quantity: 1
          }
        }()

        /** No mental rewards for simplicity */
        let mentalStateDynamics = function () {
          return {
            estimationHeuristicArr: [],
            mentalStateArr: [],
            mentalUtilities: [[]]
          }
        }()

        let initialState = []

        let params = {
          // beliefRepresentation: 'dirichlet',
          numberOfAgents: 1,
          numberOfRewards: {
            physical: 1,
            mental: 0
          }
        }

        let rewardUtilityFunctions = [identity]

        return {
          actions,
          transitionFn,
          initialState,
          physicalRewardStructure,
          mentalStateDynamics,
          rewardUtilityFunctions,
          API,
          params
        }
      }

      /** Define a basic scenario */
      let scenarios = function () {
        let agent = {
          params: {
            goalCoeffs: [1],
            metaParams: {
              alpha: 1000,
              discountFactor: 0.8,
              lookAhead: 2
            }
          },
          initialState: {
            belief: [undefined],
            mentalEstimations: [undefined],
            metaParamsEstimations: {
              alpha: [undefined],
              lookAhead: [undefined],
              discountFactor: [undefined]
            }
          }
        }

        let result = [
          {
            name: 'basic',
            agents: [ agent ],
            options: {
              horizon: 5,
              beliefRepresentation: 'dirichlet'
            }
          },
          {
            name: 'basic with discrete belief',
            agents: [ agent ],
            options: {
              horizon: 5,
              beliefRepresentation: 'discrete'
            }
          }
        ]

        return result
      }()

      let agents = ['bobby']

      let description = 'Another rather trivial test, but this time an' +
        ' agent has two available actions: *good* and *bad*. We expect he' +
        ' chooses *good* every time as he is (almost) perfectly rational'

      let validateOutput = function(trajectories) {
        assertIsArray(trajectories, OBJECT_TYPE, 2,
          "FAIL: trajectories array not as expected")
        map(function (trajectory) {
          assertIsArray(trajectory, OBJECT_TYPE, 5,
            "FAIL: trajectory: " + trajectory + "not as expected")
          mapIndexed(function (i, elem) {
            let state = repeat(i, function () {
              return 'good'
            })
            assertEqual(elem, [state, 'good'],
              'computed action not as expected')
          }, trajectory)
        }, trajectories)
      }

      return {
        makeGame,
        scenarios,
        agents,
        description,
        validateOutput
      }
    }(),

    /** THIRD TEST (one player simple with mental) */
    function() {
      let makeGame = function () {
        let actions = function (state) { return ['nice', 'mean'] }

        let transitionFn = function (state, action) {
          let nextState = [action].concat(state)
          return Delta({v: nextState})
        }

        let API = function () {
          let getPreviousState = function (state) {
            return state.slice(1)
          }
          let getLastAction = function (state) {
            assert(!isInitial(state));
            return state[0]
          }
          let isInitial = function (state) {
            return state.length == 0
          }
          let turn = getConstantFn(0)

          let stateToString = arrayToString
          let API = {
            getPreviousState,
            getLastAction,
            isInitial,
            turn,
            stateToString
          }
          return API
        }()

        let physicalRewardStructure = function () {
          let stateRewards = function (state) {
            return [[0]]
          }
          let actionRewards = function (state, action) {
            let reward = {
              'mean': 1,
              'nice': -1
            }[action]
            return [[reward]]
          }
          return {
            actionRewards,
            stateRewards
          }
        }()

        /** No mental rewards for simplicity */
        let mentalStateDynamics = function () {

          let computeMentalState = function(state, belief) {
            return {
              'mean': -1,
              'nice': 1
            }[state[0]]
          }

          return {
            estimationHeuristicArr: [undefined], /** not needed */
            mentalStateArr: [computeMentalState],
            mentalUtilities: [
/** agent 0*/ [[0]]
            ]
          }
        }()

        let initialState = []

        let params = {
          beliefRepresentation: 'dirichlet',
          numberOfAgents: 1,
          numberOfRewards: {
            physical: 1,
            mental: 1 /** = sum (map length rewardTypeArr) */
          }
        }

        let rewardUtilityFunctions = [identity, identity]

        return {
          actions,
          transitionFn,
          initialState,
          physicalRewardStructure,
          mentalStateDynamics,
          rewardUtilityFunctions,
          API,
          params
        }
      }

      /** Define a basic scenario */
      let scenarios = [
        {
          name: 'nice guy who cares about others',
          agents:
            [
              {
                params: {
                  /** format [physical goal, mental goal] */
                  goalCoeffs: [0.2,0.8],
                  metaParams: {
                    alpha: 1000,
                    discountFactor: 0.8,
                    lookAhead: 2
                  }
                },
                initialState: {
                  belief: [undefined],
                  mentalEstimations: [undefined],
                  metaParamsEstimations: {
                    alpha: [undefined],
                    lookAhead: [undefined],
                    discountFactor: [undefined]
                  }
                }
              }
            ],
          options: {
            horizon: 5,
            beliefRepresentation: 'dirichlet'
          }
        },
        {
          name: 'mean guy who cares about himself',
          agents:
            [
              {
                params: {
                  /** format [physical goal, mental goal] */
                  goalCoeffs: [0.9,0.1],
                  metaParams: {
                    alpha: 1000,
                    discountFactor: 0.8,
                    lookAhead: 2
                  }
                },
                initialState: {
                  belief: [undefined],
                  mentalEstimations: [undefined],
                  metaParamsEstimations: {
                    alpha: [undefined],
                    lookAhead: [undefined],
                    discountFactor: [undefined]
                  }
                }
              }
            ],
          options: {
            horizon: 5,
            beliefRepresentation: 'dirichlet'
          }
        }
      ]

      let agents = ['bobby']

      let description = 'Another one agent test, but this time, mental' +
        ' goal is included. Because there is only one agent, mental' +
        ' goal is agent\'s own state. We assume agent has two actions' +
        ' to choose from: \'nice\' and \'mean\' where the first one' +
        ' represents being nice to a person, resulting in lower' +
        ' physical rewards (as we\'re giving something up eg sharing)' +
        ' but higher mental rewards (feeling good about oneself).' +
        ' The second action is the opposite. We consider different' +
        ' scenarios modelling different types of agents.'

      let validateOutput = function(trajectories) {
        assertIsArray(trajectories, OBJECT_TYPE, 2,
          "FAIL: trajectories array not as expected: " +
          arrayToString(trajectories))
        let validateTrajectory = function(i, trajectory) {
          assertIsArray(trajectory, OBJECT_TYPE, 5,
            "FAIL: trajectory: " + trajectory + "not as expected")
          let arr = ['nice', 'mean']
          mapIndexed(function(j, elem) {
            let state = repeat(j, function() { return arr[i]})
            assertEqual(elem, [state, arr[i]],
              'computed action not as expected')
          }, trajectory)
        }
        mapIndexed(validateTrajectory, trajectories)
      }

      return {
        makeGame,
        scenarios,
        agents,
        description,
        validateOutput
      }
    }(),

    /** FOURTH TEST (tic tac toe - two players, no mental) */
    function() {

      let makeGame = function () {
        /** state here is represented more efficiently (hopefully)
         * taking advantage of the fact that history doesn't really matter,
         * what matters is current state */

        /** state is called 'board' below */

        let isOver = function(board) {
          return threeInARow(board, 'X') ||
            threeInARow(board, 'O') ||
            all(isDefined, board)
        }

        let indexBoard = function(board) {
          return mapIndexed(function(i, mark) {
            return [mark, i]
          }, board)
        }

        let marks = ['X', 'O']

        let getRows = function(board) {
          return map(function(i) {
            return board.slice(3*i, 3*i+3)
          }, rangeArray(0, 2))
        }

        let getCols = function(board) {
          let indexedBoard = indexBoard(board)
          map(function(colNo) {
            let indexedColumn = filter(function(indexedSquare) {
              return indexedSquare[1] % 3 === colNo
            }, indexedBoard)
            assertEqual(indexedColumn.length, 3,
              "getCols(board = " + stateToString(board) +
              "): wrong size of column")
            return map(function(indexedSquare) {
              return indexedSquare[0]
            }, indexedColumn)
          }, rangeArray(0,2))
        }

        let getDiagonals = function(board) {
          return [
            [board[0], board[4], board[8]],
            [board[2], board[4], board[6]]
          ]
        }

        let isWinning = function(line, mark) {
          return all(function(m) { return mark === m}, line)
        }

        let threeInARow = function(board, mark) {
          /** check rows */
          let rows = getRows(board)
          let cols = getCols(board)
          let diags = getDiagonals(board)
          let allLines = arrayConcat([rows, cols, diags])
          let winningLine = any(function (line) {
              return isWinning(line, mark)
            },
            allLines)
          return winningLine
        }

        /** actions are moves i.e. [mark, position] pairs */
        let actions = function (board) {
          if (isOver(board)) return ["nil"]
          let mark = marks[turn(board)]
          /** retrieve empty positions */
          let indexedSquares = indexBoard(board)
          let emptyIndexedSquares = filter(function(indexedSquare) {
            return indexedSquare[0] === undefined
          }, indexedSquares)
          let moves = map(function(emptyIndexedSquare) {
            return [mark, emptyIndexedSquare[1]]
          }, emptyIndexedSquares)
          return moves
        }

        let transitionFn = function (board, action) {
          if (action === "nil") return Delta({v: board})
          let mark = action[0]
          let pos = action[1]
          assert(isUndefined(board[pos]),
            "move " + arrayToString(action) + " in state " +
            stateToString(board) + " invalid!")
          assert(marks[turn(board)] === mark,
            "it's not turn of " + mark + " at state " +
            stateToString(board))
          let nextState = arrayReplace(board, pos, mark)
          return Delta({v: nextState})
        }


        /** represents board as follows:
         *  ___
         * |MMM|
         * |MMM|
         * |MMM|
         *
         * where each M is one of ' ', 'X', 'O'
         */
        let stateToString = function(board) {
          let short = true
          let firstLine = short ? "" : "\n ___"
          let result = reduceL(function(acc, mark) {
            let m = mark || " "
            let colNo = acc[0] % 3
            let append = short ? m : function() {
              if (colNo === 0) return "\n|" + m
              if (colNo === 2) return m + "|"
              return m
            }()
            return [acc[0]+1, acc[1] + append]
          }, [0, firstLine], board)
          return result[1] + (short ? "" : "\n")
        }

        let API = function () {
          let getPreviousState = function (state) {
            error("something's gone wrong: getPreviousState should not" +
              " be called as no mental and no belief update in this game")
          }
          let getLastAction = function (state) {
            error("something's gone wrong: getLastAction should not" +
              " be called as no mental and no belief update in this game")
          }
          let isInitial = function (state) {
            return all(isUndefined, state)
          }

          let turn = function(state) {
            let takenSquares = filter(isDefined, state)
            return takenSquares.length % 2
          }

          let API = {
            getPreviousState,
            getLastAction,
            isInitial,
            turn,
            stateToString
          }
          return API
        }()

        let physicalRewardStructure = function () {
          let stateRewards = function (state) {
            if (threeInARow(state, 'X')) return [[1],[-1]]
            if (threeInARow(state, 'O')) return [[-1],[1]]
            return [[0],[0]]
          }
          let actionRewards = function (state, action) {
            return [[0], [0]]
          }
          return {
            actionRewards,
            stateRewards
          }
        }()

        /** No mental rewards for simplicity */
        let mentalStateDynamics = function () {

          return {
            estimationHeuristicArr: [], /** not needed */
            mentalStateArr: [],
            mentalUtilities: [
              [], /** agent 0*/
              []
            ]
          }
        }()

        let initialState = repeat(9, getUndefined)

        let params = {
          numberOfAgents: 2,
          numberOfRewards: {
            physical: 1,
            mental: 0 /** = sum (map length rewardTypeArr) */
          }
        }

        let rewardUtilityFunctions = [identity]

        /** Some small tests for the more complex functions */
        let testGameStructure = function() {
          assert(!threeInARow(initialState, 'X'),
            "FAIL: threeInARow for X holds in initialState")
          assert(!threeInARow(initialState, 'O'),
            "FAIL: threeInARow for O holds in initialState")
          let state1 = sample(transitionFn(initialState, ['X',0]))
          assert(!threeInARow(state1, 'X'),
            "FAIL: threeInARow for X holds in state1")
          assert(!threeInARow(state1, 'O'),
            "FAIL: threeInARow for O holds in state1")
          let state2 = sample(transitionFn(state1, ['O',4]))
          let state3 = sample(transitionFn(state2, ['X',2]))
          assert(!threeInARow(state3, 'X'),
            "FAIL: threeInARow for X holds in state3")
          assert(!threeInARow(state3, 'O'),
            "FAIL: threeInARow for O holds in state3")
          let state4 = sample(transitionFn(state3, ['O',3]))
          let state5 = sample(transitionFn(state4, ['X',1]))
          assert(isOver(state5), "FAIL: game should be over in state5")
          assert(!threeInARow(state5, 'O'),
            "FAIL: threeInARow for O holds in state5")
          assert(threeInARow(state5, 'X'),
            "FAIL: threeInARow for X doesnt hold in state5, but should")
        }()

        return {
          actions,
          transitionFn,
          initialState,
          physicalRewardStructure,
          mentalStateDynamics,
          rewardUtilityFunctions,
          API,
          params
        }
      }

      /** Define a basic scenario */
      let scenarios = function() {
        let params = {
          /** format [physical goal, mental goal] */
          goalCoeffs: [1],
            metaParams: {
              alpha: 1000,
              discountFactor: 0.9,
              lookAhead: 9
          }
        }
        let alpha = [undefined, Delta({v: 1000})]
        let discountFactor = [undefined, Delta({v: 0.9})]
        let lookAhead = [undefined, Delta({v: 3})]
        // let metaParamsEstimations = {
        //   alpha,
        //   lookAhead,
        //   discountFactor
        // }
        let mentalEstimations = [undefined, undefined]
        /** belief over oneself is a delta */
        let belief = [
          Delta({v:["belief over oneself"]}),
          Delta({v:[1]})
        ]
        let crossAgent = {
          params,
          initialState: {
            belief,
            mentalEstimations,
            metaParamsEstimations: {
              alpha, lookAhead, discountFactor
            }
          }
        }
        let noughtAgent = {
          params,
          initialState: {
            belief: arrayReverse(belief),
            mentalEstimations,
            metaParamsEstimations: {
              alpha: arrayReverse(alpha),
              lookAhead: arrayReverse(lookAhead),
              discountFactor: arrayReverse(discountFactor)
            }
          }
        }
        // let startingState = [["X",8],["O",4],["X",0]]
        // let startingState =
        //   ['X', 'O', undefined,
        //     undefined, undefined, undefined,
        //     undefined, undefined, undefined
        //   ]
        let startingState =
          [ undefined, undefined, undefined,
            undefined, undefined, undefined,
            undefined, undefined, undefined
          ]
        let result = [
          {
            name: 'nice guy who cares about others',
            agents:
              [crossAgent, noughtAgent],
            options: {
              horizon: 1,
              beliefRepresentation: 'discrete'
            },
            startingState
          }
        ]
        return result
      }()

      let agents = ['alice','bob']

      let description = 'Tic-tac-toe is the first two player example' +
        ', it doesnt feature mental rewards so should be a little' +
        ' easier to deal with'

      let validateOutput = function(trajectories) {
        assertIsArray(trajectories, OBJECT_TYPE, 1,
          "FAIL: trajectories array not as expected: " +
          arrayToString(trajectories))
        display(trajectories[0])
        // let validateTrajectory = function(i, trajectory) {
        //   assertIsArray(trajectory, OBJECT_TYPE, 5,
        //     "FAIL: trajectory: " + trajectory + "not as expected")
        //   let arr = ['nice', 'mean']
        //   mapIndexed(function(j, elem) {
        //     let state = repeat(j, function() { return arr[i]})
        //     assertEqual(elem, [state, arr[i]],
        //       'computed action not as expected')
        //   }, trajectory)
        // }
        // mapIndexed(validateTrajectory, trajectories)
      }

      return {
        makeGame,
        scenarios,
        agents,
        description,
        validateOutput
      }
    }(),

    /** FIFTH TEST (simple trust game with fixed params) */
    function() {
      let makeGame = function () {
        /** player 0 is alice (aka investor)
         *  player 1 is bob (aka investee) */
        let endowments = [2,1]
        let investorEndowment = endowments[0]
        let k = 2

        /** Auxiliary functions */
        let endowment = function(state) {
          if (turn(state) == 0) return endowments
          return [0,0]
        }

        let transfer = function(state, action) {
          let investor = action * ((turn(state) == 0) ? -1 : 1)
          let investee = (-1) * investor
          return [investor, investee]
        }

        let maxPossibleTransfer = function(state) {
          return (turn(state) === 0) ? investorEndowment :
            state.investments[0] * k
        }

        /** game mechanics */
        let actions = function (state) {
          if (turn(state) === 0) return rangeArray(0, investorEndowment)
          assert(state.investments.length > 0,
            "investee turn but no past investments recorded")
          return rangeArray(0, state.investments[0] * k)
        }

        let transitionFn = function(state, action) {
          let turn = turn(state)
          let nextTurn = (turn + 1) % 2
          let nextInvestments = (turn === 0) ?
            [action].concat(state.investments) : state.investments
          let nextReturns = (turn === 1) ?
            [action].concat(state.returns) : state.returns
          let result = {
            turn: nextTurn,
            investments: nextInvestments,
            returns: nextReturns
          }
          info("transitionFn(state: " + stateToString(state) + ", action: " + action + ")"
            + ": computed " + stateToString(result))
          return Delta({v: result})
        }

        let API = function () {
          let getPreviousState = function (state) {
            if (isInitial(state)) return state
            let prevTurn = (turn(state) + 1) % 2
            let prevInvestments = (prevTurn === 0) ? state.investments.slice(1) : state.investments
            let prevReturns = (prevTurn === 1) ? state.returns.slice(1) : state.returns
            return {
              turn: prevTurn,
              investments: prevInvestments,
              returns: prevReturns
            }
          }
          let getLastAction = function (state) {
            assert(!isInitial(state), "Calling previousAction on initial state")
            if (state.investments.length > state.returns.length)
              return state.investments[0]
            return state.returns[0]
          }
          let isInitial = function (state) {
            return isEmpty(state.investments) && isEmpty(state.returns)
          }
          let turn = function (state) {
            return state.turn
          }

          let stateToString = function (state) {
            return "{invs: " + arrayToString(state.investments) +
              ", rets: " + arrayToString(state.returns) + "}"
          }
          let API = {
            getPreviousState,
            getLastAction,
            isInitial,
            turn,
            stateToString
          }
          return API
        }()

        let physicalRewardStructure = function () {
          let stateRewards = function(state) {
            let endowments = endowment(state)
            return [ [endowments[0]], [endowments[1]] ]
          }

          let actionRewards = function(state, action) {
            let transfers = transfer(state, action)
            return [ [transfers[0]], [transfers[1]] ]
          }

          return {
            actionRewards,
            stateRewards
          }
        }()


        let mentalStateDynamics = function () {

          let updateTrustEstimation = function(trust, agentID, state, action) {
            let maxPossibleTransfer = maxPossibleTransfer(state)
            // trust is only updated if other agent took action
            if (maxPossibleTransfer == 0 || state.turn == agentID) return trust
            let transferRatio = action / maxPossibleTransfer
            if (transferRatio > trust) {
              let trustIncreaseRatio = (transferRatio - trust) / (1 - trust)
              let lowRange = Math.log(trust) + 1
              let x = lowRange * (1 - trustIncreaseRatio) + trust * trustIncreaseRatio
              let result = Math.exp(x-1)
              assert(!_.isNaN(result) && result != undefined,
                "updateTrustValue returns " + result)
              return result
            } else {
              let trustDecreaseRatio = (trust - transferRatio) / trust
              let hiRange = Math.exp(trust) - 1
              let x = hiRange * (1 - trustDecreaseRatio) + trust * trustDecreaseRatio
              let result = Math.log(x+1)
              assert(!_.isNaN(result) && result != undefined,
                "updateTrustValue returns " + result)
              return result
            }
          }

          let computeTrust = function(state, belief) {
            let towardAgentID = belief.value[0] === undefined ? 1 : 0
            let individualBelief = retrieveBeliefOver(towardAgentID, belief)
            return goalCoeffExpectation(individualBelief, 1)
          }

          return {
            estimationHeuristicArr: [updateTrustEstimation], /** not needed */
            mentalStateArr: [computeTrust],
            mentalUtilities: [
              [[1]], /** agent 0 */
              [[0]]  /** agent 1 */
            ]
          }
        }()

        let initialState = {
          turn: 0,
          investments: [],
          returns: []
        }

        let params = {
          numberOfAgents: 2,
          numberOfRewards: {
            physical: 1,
            mental: 1 /** = length (mentalStateArr)
                          = length (estimationHeuristicArr) */
          }
        }

        let rewardUtilityFunctions = function() {
          let moneyUtility = function(x) {
            return x
          }

          let trustUtility = function(x) {
            return 15 * x
          }

          return [moneyUtility, trustUtility]
        }()

        return {
          actions,
          transitionFn,
          initialState,
          physicalRewardStructure,
          mentalStateDynamics,
          rewardUtilityFunctions,
          API,
          params
        }
      }

      let scenarios = function() {
        let goalCoeffs = [0.2, 0.8]
        let metaParams = {
          alpha: 100,
          discountFactor: 0.8,
          lookAhead: 2
        }
        let params = {
          /** format [physical goal, mental goal] */
          goalCoeffs,
          metaParams
        }
        let alpha = [
          undefined,
          Categorical({vs: [100]})
        ]
        let lookAhead = [
          undefined,
          Categorical({vs: [2]})
        ]
        let discountFactor = [
          undefined,
          Categorical({vs: [0.8]})
        ]
        let metaParamsEstimations = {
          alpha,
          lookAhead,
          discountFactor
        }
        let mentalEstimations = [
            undefined,
            [Delta({v: 0.7})]
          ]
        let belief = [
          undefined, /** over agent 0 */
          [1,3] /** over agent 1 (investee) */
        ]
        let alice = {
          params,
          initialState: {
            belief,
            mentalEstimations,
            metaParamsEstimations
          }
        }
        let bob = {
          params,
          initialState: {
            belief: arrayReverse(belief),
            mentalEstimations: arrayReverse(mentalEstimations),
            metaParamsEstimations: {
              alpha: arrayReverse(alpha),
              lookAhead: arrayReverse(lookAhead),
              discountFactor: arrayReverse(discountFactor)
            }
          }
        }
        let result = [
          {
            name: 'basic: high trustworhty, high trust, perfect info',
            agents:
              [ alice, bob ],
            options: {
              horizon: 5,
              beliefRepresentation: 'dirichlet'
            }
          }
        ]
        return result
      }()

      let agents = ['alice', 'bob']

      let description = 'This is a basic trust game'

      let validateOutput = function(trajectories) {
        assertIsArray(trajectories, OBJECT_TYPE, 1,
          "FAIL: trajectories array not as expected: " +
          arrayToString(trajectories))
        // let validateTrajectory = function(i, trajectory) {
        //   assertIsArray(trajectory, OBJECT_TYPE, 5,
        //     "FAIL: trajectory: " + trajectory + "not as expected")
        //   let arr = ['nice', 'mean']
        //   mapIndexed(function(j, elem) {
        //     let state = repeat(j, function() { return arr[i]})
        //     assertEqual(elem, [state, arr[i]],
        //       'computed action not as expected')
        //   }, trajectory)
        // }
        // mapIndexed(validateTrajectory, trajectories)
      }

      return {
        makeGame,
        scenarios,
        agents,
        description,
        validateOutput
      }
    }()

  ]

  let runSingle = function(i) {
    assert(i >= 0 && i < testsArray.length,
      "index " + i + " of test invalid")
    let test = testsArray[i]
    display(stringInABox("Test " + (i+1)))
    display(test.description)
    let trajectories =
      simulateScenarios(test.scenarios, test.makeGame, test.agents, test.callbacks)
    let validate = test.validateOutput
    validate(trajectories)
    display("PASS: Test " + (i+1))
  }

  let runAll = function() {
    map(runSingle, rangeArray(0, testsArray.length-1))
    return "all passed, congrats"
  }

  return { runAll, runSingle }
}()

let runAll = tests.runAll
let runSingle = tests.runSingle
// runAll()
runSingle(4)