/**
 * Bravery game
 * This is a simple one shot, two-player scenario in which only
 * player 1 is active. They have two actions to choose from: 'bold' and
 * 'timid'. The idea is that apart from their internal preference
 * for being timid, they are also being motivated by pleasing
 * their friends, modeled as player 2. To please their friends, player
 * 1 must estimate what they expect him/her to do and if they expect
 * bold, she/he's better of being bold, otherwise timid.
 *
 * On the other hand, friends (player 2) prefer their friend being
 * bold and moreover, they prefer to *think* of him as bold.
 *
 * This scenario is modeled using psychological games formalism and
 * three equilibria are found:
 * - bold
 * - timid
 * - 1/2 -> bold, 1/2 -> timid
 * Since beliefs are assumed accurate in equilibrium, strategy of
 * player 1 determines beliefs in equilibrium.
 *
 * We propose to model this scenario as a CSMG and offer insights
 * complementary to analysis via psychological games.
 * Our primary observation is that equilibrium behaviour may be
 * reached after repeated play, but is unlikely to arise from the
 * onset. Therefore, our model can be used to explain what happens
 * when those beliefs are inaccurate and when/if equilibrium is
 * reached.
 *
 * We model this scenario as follows:
 * Boldness of an agent will be modeled using goal coefficients.
 * In particular, there's one reward structure that assigns a unit
 * reward for being bold and another one that assigns a unit reward
 * for being timid. Adjusting goal coefficients corresponding to those
 * rewards makes an agent more bold or more timid.
 *
 * Additionally, we have a single mental reward structure that models
 * pride experienced by friends (player 2) following player 1's
 * decision. This pride is computed based on belief, detailed in
 * method *computePride* below. Now, player 1's motivation is dependent
 * on their second order belief, which is unsupported by our framework.
 * Instead, we assume that player 1 estimates pride of his/her friends
 * and gains their utility from that estimation.
 */


let makeCSMG = function(gameSpecificParams) {

  /**
   * Now come functions that describe the mechanics of the game
   */

  /** Actions available to an agent (the owner of *state*
   * - assumed to be unique) in *state* */
  let actions = function(state) {
    return ["bold", "timid"]
  }

  /**  probabilistic transition function */
  let transitionFn = function(state, action) {
    let nextState = [action].concat(state)
    return Delta({v: nextState})
  }

  /**
   * Now comes the "API" of the game.
   * This is a set of functions which are used by our library but
   * whose implementation is game-specific.
   * */
  let API = function() {
    let getPreviousState = function (state) {
      if (isInitial(state)) return state
      return state.slice(1)
    }

    /** return action that was taken to get to *state* */
    let getLastAction = function (state) {
      assert(!isInitial(state), "Calling previousAction on initial state")
      return state[0]
    }

    let isInitial = function (state) {
      return state.length === 0
    }

    let turn = function (state) {
      return 0
    }

    /** returns the string representation of *state*.
     * (for debugging purposes) */
    let stateToString = function (state) {
      return arrayToString(state)
    }

    let API = {
      getPreviousState,
      getLastAction,
      isInitial,
      turn,
      stateToString
    }

    return API
  }()

  /**
   *  Now comes physical reward structure.
   *  This should typically be a simple component capturing what,
   *  and how much, physical rewards (money, time, number of sweets
   *  etc) agents receive in each state
   */
  let physicalRewardStructure = function() {

    /** Physical rewards gained by each agent at *state*
     * Should return an array indexed by agentID
     */
    let stateRewards = function(state) {
      return [ [1,1], [1,1]]
    }

    /** As above but for action rewards
     */
    let actionRewards = function(state, action) {
      return [ [0,0], [0,0]]
    }

    return {
      actionRewards,
      stateRewards
    }
  }()

  /**
   *  Now comes mental state dynamics model. That's the most important
   *  component, it captures the mental quantities (such as trust, guilt,
   *  pleasure, fairness, reciprocity).
   *  It consists of two components:
   *  (i) Heuristics agents use to estimate mental state of their opponents.
   *    This should be specified as an array of update functions, one for each
   *    mental state. Each update function has
   *    @type (mentalStateValue, estimatingAgent, estimatedAgent, state, action) -> newMentalStateValue
   *  (ii) Mental state computation, i.e. how can actual mental state of an
   *    agent be computed. Each such function that computes some mental state
   *    of an agent has
   *    @type (state, belief) -> mentalStateValue
   */
  let mentalStateDynamics = function() {

    /** Heuristic is defined by 'update functions' which capture
     * how *estimatingAgent*'s estimation of opponent's (*estimatedAgent*)
     * mental attitude changes as a result of *action* taken in *state*
     */
    let updatePrideEstimation =
      function(prevValue, estimatingAgentID, estimatedAgentID, state, observation) {
        if (_.isEqual(observation, "frown")) {
          return prevValue
        }
        return prevValue
      }

    let computePride = function(agentID, belief, state) {
      if (agentID === 0) return 0
      let individualBelief = retrieveBeliefOver(0, belief)
      let boldCoeff = goalCoeffExpectation(individualBelief, 0)
      let timidCoeff = goalCoeffExpectation(individualBelief, 1)
      let sum = boldCoeff + timidCoeff
      let boldBelief = boldCoeff / ((1 + sum)/2)
      assert(boldBelief >= 0 && boldBelief <= 1,
        "computePride(): boldBelief expected in [0,1]; found: "
        + boldBelief)
      let getLastAction = API.getLastAction
      let lastAction = getLastAction(state)
      if (_.isEqual(lastAction, 'bold')) return boldBelief
      return -boldBelief/2
    }

    return {
      estimationHeuristicArr: [ updatePrideEstimation ],
      mentalStateArr: [ computePride ],
      mentalUtilities: [
        [[1]], /** agent 0 */
        [[1]] /** agent 1 */
      ]
    }
  }()

  let initialState = []

  let params = {
    numberOfAgents: 2,
    numberOfRewards: {
      physical: 2,
      mental: 1
    }
  }

  let rewardUtilityFunctions = function() {
    let timesTwo = function(x) {
      return 2*x
    }
    return [identity, identity, timesTwo]
  }()

  return {
    actions,
    transitionFn,
    initialState,
    physicalRewardStructure,
    mentalStateDynamics,
    rewardUtilityFunctions,
    API,
    params
  }
}