/** Cognitive trust aware agent

 * As described in the paper, an agent is characterised by
 * 1. an array of coefficients
 *    *goalCoeffs* = [a1,a2,...,an]
 *    (we require a1 + a2 + ... + an = 1)
 * 2. meta-parameters
 *    *metaParams* = {
 *      alpha: [0,inf),
 *      lookAhead: [0, inf) (integer),
 *      discountFactor: (0,1]
 *    }
 * The above dictionaries are stored in a single dict *selfParams*
 * Moreover, each agent has a role (*selfId*), which is 'investor' or 'investee'
 * Next, each agent has an initial state, consisting of
 * 3. initial belief about opponent's mental characteristics.
 *   Currently assumed to be Dirichlet distribution and represented by
 *   its set of parameters. However, implementation should be representation-agnostic:
 *   we provide certain operations on belief:
 *   - updateBelief
 * 4. initial estimations of mental rewards of agent's opponent.
 *    At the minimum, this contains estimation of trust.
 *    It is an array.
 *    Eg [<trustEstimation:distribution>]
 * 5. estimation of opponent's mental parameters
 *    metaParamsEstimations = distribution over values
 {
        alpha: <discrete dist>,
        lookAhead: <discrete dist>,
        discountFactor: <discrete dist>
      }
 Hence, *initialState* takes a form of a following dictionary {
     belief: <3>,
     mentalEstimations: <4>,
     metaParamsEstimations: <5>
   }
 *
 */
// TODO: could be nice here if gamespecific api was allowed to assert params
//  passed are valid (eg selfid needs to be investor of investee)
let makeAgent = function (selfParams, selfId, initialState, game) {
  /** Extract some functions from the game API for easy access */
  let gameAPI = assertDefined(game.API, "makeAgent(): game.API undefined ")
  let isInitial = assertDefined(gameAPI.isInitial,
    "makeAgent(): gameAPI.isInitial undefined")
  let getPreviousState = assertDefined(gameAPI.getPreviousState,
    "makeAgent(): gameAPI.getPreviousState undefined")
  let getLastAction = assertDefined(gameAPI.getLastAction,
    "makeAgent(): gameAPI.getLastAction undefined")
  let stateToString = assertDefined(gameAPI.stateToString,
    "makeAgent(): gameAPI.stateToString undefined")
  let turn = assertDefined(gameAPI.turn,
    "makeAgent(): gameAPI.turn undefined")
  let actionSimilarity = assertDefined(gameAPI.actionSimilarity,
    "makeAgent(): gameAPI.actionSimilarity undefined")

  /** Extract some fields for easier access */
  let transitionFn = game.transitionFn
  let actions = game.actions
  let utilityFn = game.utilityFn
  let utilityAtIndex = game.utilityAtIndex

  let gameParams = assertDefined(game.params,
    "makeAgent(): game.params undefined")
  let beliefRepresentation = gameParams.beliefRepresentation
  let numberOfAgents = gameParams.numberOfAgents
  let numberOfRewards = gameParams.numberOfRewards
  let goalCoeffsNumberByAgent = gameParams.goalCoeffsNumberByAgent

  assertDefined(selfId, "makeAgent(): selfId not given")
  /** Meta-parameters */
  assertDefined(selfParams, "makeAgent: selfParams not passed")
  let selfMetaParams = assertDefined(selfParams.metaParams,
    "makeAgent: metaParams missing")
  assertDefined(selfMetaParams.lookAhead, "makeAgent: lookAhead missing")
  assertDefined(selfMetaParams.alpha, "makeAgent: alpha missing")
  assertDefined(selfMetaParams.discountFactor, "makeAgent: discountFactor missing")

  /** Initial state */
  assertDefined(initialState, "makeAgent: initialState missing")
  let initialBeliefValue = assertDefined(initialState.belief,
    "makeAgent: initialBelief missing")
  let initialBelief = {
    representation: beliefRepresentation,
    value: initialBeliefValue
  }
  let initialMentalEstimations = assertDefined(initialState.mentalEstimations,
    "makeAgent: initialMentalEstimations missing")
  let metaParamsEstimations = assertDefined(initialState.metaParamsEstimations,
    "makeAgent: metaParamsEstimations missing")
  assertDefined(metaParamsEstimations.alpha,
    "makeAgent: metaParamsEstimations.alpha missing")
  assertIsArray(metaParamsEstimations.alpha, OBJECT_TYPE, numberOfAgents,
    "makeAgent: metaParamsEstimations.alpha is not as expected: "
    + toString(metaParamsEstimations.alpha))
  assertDefined(metaParamsEstimations.lookAhead,
    "makeAgent: metaParamsEstimations.lookAhead missing")
  assertIsArray(metaParamsEstimations.lookAhead, OBJECT_TYPE, numberOfAgents,
    "makeAgent: metaParamsEstimations.lookAhead is not as expected: "
    + toString(metaParamsEstimations.lookAhead))
  assertDefined(metaParamsEstimations.discountFactor,
    "makeAgent: metaParamsEstimations.discountFactor missing")
  assertIsArray(metaParamsEstimations.discountFactor, OBJECT_TYPE, numberOfAgents,
    "makeAgent: metaParamsEstimations.discountFactor is not as expected: "
    + toString(metaParamsEstimations.discountFactor))

  /** Reward structures */
  let getPhysicalRewardStructure = game.getPhysicalRewardStructure
  let physicalRewardStructure = assertDefined(getPhysicalRewardStructure(),
    'physicalRewardStructure undefined')
  let stateRewards = physicalRewardStructure.stateRewards
  let actionRewards = physicalRewardStructure.actionRewards

  let getMentalRewardStructure = game.getMentalRewardStructure
  let mentalRewardStructure = assertDefined(getMentalRewardStructure(initialMentalEstimations, selfId),
    'mentalRewardStructure undefined')
  let computeMentalRewards = mentalRewardStructure.computeMentalRewards
  let computeMentalState = mentalRewardStructure.computeMentalState
  let mentalUtilities = mentalRewardStructure.mentalUtilities

  /** Goal coefficients of this agent */
  let selfGoalCoeffs = selfParams.goalCoeffs
  let selfPhysicalGoalCoeffs = selfGoalCoeffs.slice(0, numberOfRewards.physical)
  let selfMentalGoalCoeffs = selfGoalCoeffs.slice(numberOfRewards.physical)

  /** Optional: agent game specific parameters */
  let numberOfAgentParams = gameParams.numberOfAgentParams
  if (numberOfAgentParams !== undefined) {
    let agentParams = selfParams.agentParams
  }

  /*********************
   * Belief operations *
   *********************
   Abstractly, belief is a continuous probability distribution.
   Concretely, two finite representations are proposed:
   - a discrete distribution that approximates a continuous distribution
   - a set of parameters of dirichlet distribution; here we make an
   assumption that belief follows dirichlet dist
   We would like to be able to change between those, and possibly more,
   representations easily, while exposing a uniform interface for the
   rest of the code that uses belief.
   We therefore include the following methods in the 'interface' of belief
   - belief(state)
   - updateBelief(belief, observation)
   */

  /** Belief
   *  Computes selfID's belief of ofAgentID in *state*. Proceeds recursively.
   *  To avoid recomputation, this function is cached.
   */
  let belief = dp.cache(function (state) {
    info("belief(): state: " + stateToString(state) + ", of agent " + selfId)
    let opponentsHaveNoMoreThanOneReward = all(function(elem) { return elem <= 1 },
      arrayReplace(goalCoeffsNumberByAgent, selfId, 0))
    if (isInitial(state) || opponentsHaveNoMoreThanOneReward) {
      info("belief(): returning initial")
      return initialBelief
      // return retrieveBeliefOf(ofAgentId, initialBelief)
    }
    let prevState = getPreviousState(state)
    let lastAction = getLastAction(state)
    let prevBelief = belief(prevState)
    let result = updateBelief(prevBelief, prevState, lastAction)
    info("belief(state: " + stateToString(state) + "): returning " +
      beliefToString(result, selfId, goalCoeffsNumberByAgent))
    return result
  })

  /** belief here is over individual agent already */
  let updateBelief = function (belief, state, action) {
    let updateBeliefDiscrete = function (beliefVal, actingAgentID) {
      info(stringInABox("updateBeliefDiscrete(): action=" + action))
      let updatedIndividualBelief = Infer({method: 'enumerate'}, function () {
        let beliefOverActingAgent = retrieveBeliefOver(actingAgentID, belief)
        let goalCoeffs = sampleBelief(beliefOverActingAgent)
        let cond = {
          representation: belief.representation,
          value: goalCoeffs
        }
        // let act = agents[agentID].act
        let predictedAction = sample(act(state, cond))
        factor(actionSimilarity(state, predictedAction, action))
        return goalCoeffs
      })
      /** update belief over that agent */
      let updatedBeliefValue = arrayReplace(belief.value, actingAgentID, updatedIndividualBelief)
      return {
        representation: 'discrete',
        value: updatedBeliefValue
      }
      /** for now lets assume we dont update belief */
      // return belief
    }

    /**
     * *belief* here is an array of parameters (of a dirichlet distribution)
     * The idea of an update is as follows: for each reward (physical or mental)
     * we compute the probability of agent taking *action* assuming they're solely
     * motivated by that reward. We then increment the distribution parameter corresponding
     * to that reward by the value of probability computed.
     */
    let updateBeliefDirichlet = function (beliefVal, actingAgentID) {
      info("updateBeliefDirichlet(): actingAgentID=" + actingAgentID + ", belief:")
      info(arrayToString(belief.value))
      /** compute the action under each possible reward, i.e. conditional action */
      let computeActionDist = function (index) {
        /** A 'conditional' call to act() in a sense that certain conditions are
         * placed on the execution of that call. In particular:
         * - value of trust computed based on *belief* is to be used for the purposes
         *    of computing utility of this agent in recursive act calls
         * - opponent is assumed to only care about reward number *index*
         */
        let cond = {
          representation: 'dirichlet',
          value: index
        }
        // let act = agents[agentID].act
        return act(state, cond) /** index is the condition */
      }
      let actingAgentsGoalCoeffIndexes =
        rangeArray(0, goalCoeffsNumberByAgent[actingAgentID]-1)
      let actionDists = map(computeActionDist, actingAgentsGoalCoeffIndexes)
      let actionProbs = map(function (actionDist) {
        return Math.exp(actionDist.score(action))
      }, actionDists)
      // action probs might be very small; instead of adding them directly, we use them
      // as proportions and add a total of one
      let actionProbsSum = sum(actionProbs)
      let actionProbsNormalised = map(function (prob) {
        return (actionProbsSum === 0) ? 0 : prob / actionProbsSum
      }, actionProbs)
      assert(actionProbsSum === 0 || approxEqual(sum(actionProbsNormalised), 1),
        "Normalised action probabilities in updateBeliefDirichlet " +
        "don't sum to 1; found: " + sum(actionProbsNormalised))
      let updatedBeliefOverActingAgent =
        map2(add, beliefVal[actingAgentID], actionProbsNormalised)
      let updatedBelief = arrayReplace(beliefVal, actingAgentID, updatedBeliefOverActingAgent)
      info("updateBeliefDirichlet(): returning updatedBelief (value): ")
      info(arrayToString(updatedBelief))
      return {
        representation: 'dirichlet',
        value: updatedBelief
      }
    }

    info("updateBelief(): belief=" + toString(belief) + ", state=" +
      stateToString(state) + ", action=" + toString(action))
    let appropriateFn = {
      'discrete': updateBeliefDiscrete,
      'dirichlet': updateBeliefDirichlet
    }[belief.representation]

    let actingAgentID = turn(state)
    if (selfId == actingAgentID || actions(state).length == 1) {
      return belief
    }
    return appropriateFn(belief.value, actingAgentID)
    /** again, for now assume no update for simpl */
    // return belief
  }

  /** Utility function
   * Computes utility for agent *role* when *action* is performed (by whoever) in *state*.
   * Physical rewards are computed using physical reward structure
   * 1. If utility of this agent is being computed, mental rewards are computed
   * using mental rewards structure (dynamics functions) and goal coefficients are known
   * 2. Otherwise, there are two cases:
   * 2a. When no condition *cond* is passed then opponent's utility is computed with respect
   *     to this agent's belief and mental rewards are taken as real values this agent knows
   * 2b. When condition *cond* is passed, utility is computed wrt
   * This is achieved by computing income as a sum of endowment and transfer (can be negative)
   * and trust value using trust dynamics.
   */
  let actionUtility = dp.cache(function (state, action, ofAgentID, cond, mentalSnapshot) {
    info("actionUtility(state: " + stateToString(state) + ", action: " + action +
      ", role: " + ofAgentID + ", cond: " + condToString(cond) + ")")
    assertDefined(mentalSnapshot, "actionUtility: mentalSnapshot undefined")

    let rewards = actionRewards(state, action)[ofAgentID]
    let mentalRewardsDummy = mentalUtilities[ofAgentID]
    // let mentalRewardsDummy = repeat(numberOfRewards.mental, function () {
    //   return 0
    // })
    if (ofAgentID == selfId) {
      // case 1
      info("actionUtility(): case 1: agent computes their own utility")
      return utilityFn(selfPhysicalGoalCoeffs, rewards)
    }
    if (cond !== undefined) {
      // case 2b
      info("actionUtility(): case 2b: agent computes other's utility on cond")
      return conditionalUtility(rewards.concat(mentalRewardsDummy), cond, utilityFn, utilityAtIndex)
    }
    info("actionUtility(): case 2a: agent computes other's utility")
    // TODO: which one should be used?
    // var belief = belief(state)
    let belief = retrieveBeliefOver(ofAgentID, belief(mentalSnapshot.state))

    // case 2a
    let expectations =
      goalCoeffsExpectation(belief, goalCoeffsNumberByAgent[ofAgentID]).slice(0, numberOfRewards.physical)
    return utilityFn(expectations, rewards)
  })

  let stateUtility = dp.cache(function (state, ofAgentID, cond, mentalSnapshot) {
    info("stateUtility(state: " + stateToString(state) + ", role: " + ofAgentID + ", cond: " + condToString(cond) + ")")
    assertDefined(mentalSnapshot,
      "actionUtility: mentalSnapshot undefined")

    let belief = belief(mentalSnapshot.state)
    let physicalRewards = stateRewards(state)[ofAgentID]
    if (ofAgentID == selfId) {
      info("stateUtility(): case 1")
      // case 1
      let mentalRewards = computeMentalRewards(state, ofAgentID, belief)
      info("stateUtility(): " + selfId + " computed rewards of " + ofAgentID +
        ": " + arrayToString(mentalRewards))
      return utilityFn(selfGoalCoeffs, physicalRewards.concat(mentalRewards))
    }

    if (cond !== undefined) {
      info("stateUtility(): case 2b")
      let mentalRewards = computeMentalRewards(state, ofAgentID, belief, mentalSnapshot)
      let result = conditionalUtility(physicalRewards.concat(mentalRewards), cond, utilityFn, utilityAtIndex)
      return result
    }

    // case 2a
    info("stateUtility(): case 2a")
    // let belief = belief(mentalSnapshot.state)
    let mentalRewards = computeMentalRewards(state, ofAgentID, belief, mentalSnapshot)
    let ofAgentBelief = retrieveBeliefOver(ofAgentID, belief)
    let expectations = goalCoeffsExpectation(ofAgentBelief, goalCoeffsNumberByAgent[ofAgentID])
    let result = utilityFn(expectations, physicalRewards.concat(mentalRewards))
    info("stateUtility: returning " + result)
    return result
  })

  /** Compute action (distribution) at state
   * It either computes the action (distribution) for this agent (when no @cond
   * passed) or action (distribution) of opponent assuming @cond.
   *
   * @param state
   * @param cond (optional)
   * @return
    a distribution over actions available to the acting agent at @state
   */
  let act = dp.cache(function (state, condOpt) {
    info("act(state: " + stateToString(state) + ", cond: " + condToString(condOpt) + ")")
    let turn = turn(state)
    let thisAgentActs = turn === selfId
    explain("Agent " + selfId + " computes action at state " +
      stateToString(state), thisAgentActs && !INFERENCE_MODE)
    assert(thisAgentActs || condOpt !== undefined,
      "Input to act() doesn't meet precondition: condition must be passed")
    /** prepare mentalSnapshot for future utility computations) */
    let belief = belief(state)
    let mentalState = computeMentalState(state, belief)
    let mentalSnapshot = {
      values: mentalState,
      state
    }
    globalStore.indent = 0
    explain("Agent will explore the game tree up to depth " +
      selfMetaParams.lookAhead, thisAgentActs && !INFERENCE_MODE)
    let actionDist = Infer({method: 'enumerate'}, function () {
      let othersMetaParams = sampleMetaParamsEstimations(metaParamsEstimations)
      let allMetaParams = mergeMetaParams(othersMetaParams, selfMetaParams, selfId)
      let lookAhead = allMetaParams.lookAhead[turn]
      let actionDist = actRec(state, lookAhead, allMetaParams, condOpt, mentalSnapshot)
      return sample(actionDist)
    })
    let actionDistAsString = discreteDistributionToString(actionDist, actions(state))
    // explain("Agent " + selfId + " computed the following action distribution:\n" +
    //   actionDistAsString, thisAgentActs)
    return actionDist
  })

  /**
   * Params:
   *   - *state* - a dictionary { turn, returns, investments }
   *   - *timeLeft* - an integer specifying time horizon of the decision making
   *   - *otherMetaParamsArr* - a dict consisting of arrays of opponents'
   *   meta-params (compulsory) and goal coefficients (optional)
   *   - *cond* - is an optional condition, used for belief update
   *   - *topLevel* - an extra optional argument for debugging purposes
   */
  let actRec = dp.cache(function (state, timeLeft, allMetaParams, cond, mentalSnapshot) {
    debug(spaces(globalStore.indent) + "actRec(): state=" + stateToString(state) +
      ", horizon: " + timeLeft + ")", !INFERENCE_MODE)
    let turn = turn(state)
    let alpha = assertDefined(allMetaParams.alpha[turn],
      "bad game specification: agent's " + selfId + " estimation " +
      "of agent's " + turn + " alpha undefined")
    let actionDist = Infer({method: 'enumerate'}, function () {
      let availableActions = assertDefined(actions(state),
        "bad game specification: actions at state " +
        stateToString(state) + " undefined")
      assertIsArray(availableActions, ANY_TYPE, -1,
        "bad game specification: actions at state " +
        stateToString(state) + " are not an array, found: " +
        toString(availableActions))
      // let action = timeLeft !== 9 ? uniformDraw(availableActions) : ["X",0]
      let action = uniformDraw(availableActions)
      debug("actRec(): computing expected utility for action "
        + action, globalStore.indent === 0 && !INFERENCE_MODE)
      let eu = expectedUtility(state, action, turn, timeLeft, allMetaParams, cond, mentalSnapshot)
      factor(alpha * eu)
      return action
    })
    return actionDist
  })

  /** Computes expected utility of agent with *role* upon *action* taken in *state*
   * with *timeLeft* left. *timeLeft* must be > 0!
   * Params:
   *   - *state* - a dictionary describing state, i.e. turn, returns and investments
   *   - *action* - a string identifying action taken in *state*
   *   - *role* - a string identifying the agent whose expected utility is to
   be computed
   *   - *timeLeft* - integer
   *   - *otherParams* -
   */
  let expectedUtility = dp.cache(
    function (state, action, ofAgentID, timeLeft, allMetaParams, cond, mentalSnapshot) {
      debug(spaces(globalStore.indent) + "expectedUtility() at state " +
        stateToString(state) + ", for action: " + action + ", computed by " +
        selfId + " for " + ofAgentID + ", with time horizon " + timeLeft +
        ", condition " + (cond === undefined ? "undefined" : cond.value), !INFERENCE_MODE)
      let u = actionUtility(state, action, ofAgentID, cond, mentalSnapshot)
      let nextTimeLeft = timeLeft - 1
      let discountFactor = assertDefined(allMetaParams.discountFactor[ofAgentID],
        "bad game specification: agent's " + selfId + " estimation " +
        "of agent's " + turn + " discountFactor undefined")
      globalStore.indent += 2
      let futureUtilityDist = Infer({method: 'enumerate'}, function () {
        let nextStateDist = transitionFn(state, action)
        assertHasType(nextStateDist, DIST_TYPE,
          "bad game specification: transitionFn must return a distribution, but " +
          "returned " + toString(nextStateDist) + " when called at state " +
          stateToString(state) + " with action " + action)
        let nextState = sample(nextStateDist)
        let nextStateUtility = stateUtility(nextState, ofAgentID, cond, mentalSnapshot)
        if (nextTimeLeft == 0) {
          return nextStateUtility
        }
        let nextTurn = turn(nextState)
        let nextActionTakerHorizon = min(allMetaParams.lookAhead[nextTurn], nextTimeLeft)
        let nextActionDist = actRec(nextState, nextActionTakerHorizon, allMetaParams, cond, mentalSnapshot)
        let nextAction = sample(nextActionDist)
        return nextStateUtility + expectedUtility(nextState, nextAction,
          ofAgentID, nextTimeLeft, allMetaParams, cond, mentalSnapshot)
      })
      globalStore.indent -= 2
      let futureUtilExp = expectation(futureUtilityDist)
      let eu = u + discountFactor * futureUtilExp
      explain("EU of " + ofAgentID + " at state " + stateToString(state) +
        " for action " + action + ", with " + timeLeft + " steps left: "
        + eu, (selfId === 1 && ofAgentID === 0 && _.isEqual(state.investments, [1]) && _.isEqual(state.returns, [1]))
          || (!INFERENCE_MODE && (ofAgentID === selfId && cond === undefined && globalStore.indent === 0)))
      return eu
    })

  let mentalRewards = function(state) {
    let belief = belief(state)
    // TODO: maybe change that later
    return arrayConcat(computeMentalRewards(state, selfId, belief))
  }

  let mentalState = function(state) {
    let belief = belief(state)
    return computeMentalState(state, belief)
  }

  let getStateUtility = function(state) {
    return stateUtility(state, selfId, undefined, { state })
  }

  let getActionUtility = function(state, action) {
    return actionUtility(state, action, selfId, undefined, { state })
  }

  return {
    params: selfParams,
    act,
    expectedUtility,
    belief,
    mentalRewards,
    mentalState,
    getStateUtility,
    getActionUtility
  }
}