let getGameAPI = function (gameSpecificAPI) {

  let stateToString = gameSpecificAPI.stateToString

  let mentalSnapshotToString = function (mentalSnapshot) {
    let state = stateToString(mentalSnapshot.state)
    let values = mentalSnapshot.values
    return "{state: " + state + ", values: " + values + "}"
  }

  /** This implements default action similarity measure (which we could
   * call 'discrete measure') in case a measure is not provided in
   * user game specification */
  let actionSimilarity = function(state, a1, a2) {
    assertDefined(state, "actionSimilarity(): state undefined")
    assertDefined(a1, "actionSimilarity(): a1 undefined")
    assertDefined(a2, "actionSimilarity(): a2 undefined")
    let actionSimilarityOpt = gameSpecificAPI.actionSimilarity
    if (isDefined(actionSimilarityOpt)) return actionSimilarityOpt(state, a1,a2)
    if (_.isEqual(a1,a2)) return 0
    return -100
  }

  let APIExtension = {
    mentalSnapshotToString,
    actionSimilarity
  }

  let gameAPI = extend(gameSpecificAPI, APIExtension)
  return gameAPI
}


/** makeCSMG (cognitive stochastic multiplayer game)
 * @param gameSetup
 *  an object of the form
 *  {
    actions,
    transition,
    initialState,
    API,
    getPhysicalRewards,
    getMentalStateDynamics,
    utilityFn,
    params
    }
 * This object contains all the game specific information needed
 * to create a CSMG and it is defined by the user,
 * @param externalParams
 * of the form
 * {
 *   beliefRepresentation
 * }
 *
 * Given a game configuration, this generic function creates a
 * cognitive model. This mostly involves creating an appropriate
 * mental reward structure.
 */
let makeCSMG = function (gameSetup, externalParams) {
  /** Extract basic components and validate user-defined *gameSetup* */
  assertDefined(gameSetup,
    "makeCSMG(): gameSetup undefined")
  assertDefined(externalParams,
    "makeCSMG(): externalParams undefined")
  let params = assertDefined(gameSetup.params,
    "makeCSMG(): gameSetup.params undefined")
  let beliefRepresentation = assertDefined(externalParams.beliefRepresentation,
    "makeCSMG(): params.beliefRepresentation undefined")
  let numberOfAgents = assertDefined(params.numberOfAgents,
    "makeCSMG(): params.numberOfAgents undefined")
  let numberOfRewards = assertDefined(params.numberOfRewards,
    "makeCSMG(): params.numberOfRewards undefined")
  assertDefined(numberOfRewards.physical,
    "makeCSMG(): numberOfRewards.physical undefined")
  assertDefined(numberOfRewards.mental,
    "makeCSMG(): numberOfRewards.mental undefined")

  let actions = assertDefined(gameSetup.actions,
    "makeCSMG(): gameSetup.actions undefined")
  let transitionFn = assertDefined(gameSetup.transitionFn,
    "makeCSMG(): gameSetup.transitionFn undefined")
  let initialState = assertDefined(gameSetup.initialState,
    "makeCSMG(): gameSetup.params undefined")

  /** Extract game-specific API calls for easy access */
  let API = getGameAPI(gameSetup.API)

  let getPreviousState = assertDefined(API.getPreviousState,
    "makeCSMG(): API.getPreviousState undefined")
  let getLastAction = assertDefined(API.getLastAction,
    "makeCSMG(): API.getLastAction undefined")
  let isInitial = assertDefined(API.isInitial,
    "makeCSMG(): API.isInitial undefined")
  let stateToString = assertDefined(API.stateToString,
    "makeCSMG(): API.stateToString undefined")

  /** Reward structures */
  let physicalRewardStructure = assertDefined(gameSetup.physicalRewardStructure,
    "makeCSMG(): gameSetup.physicalRewardStructure undefined")
  let mentalStateDynamics = assertDefined(gameSetup.mentalStateDynamics,
    "makeCSMG(): gameSetup.mentalStateDynamics undefined")
  let estimationHeuristicsArr = assertDefined(mentalStateDynamics.estimationHeuristicArr,
    "makeCSMG(): estimationHeuristicsArr undefined")
  let mentalStateArr = assertDefined(mentalStateDynamics.mentalStateArr,
    "makeCSMG(): mentalStateArr undefined")
  let mentalUtilities = assertDefined(mentalStateDynamics.mentalUtilities,
    "makeCSMG(): mentalUtilities undefined")
  assertIsArray(estimationHeuristicsArr, FUNCTION_TYPE, numberOfRewards.mental,
    "makeCSMG(): estimationHeuristicArr: " + arrayToString(estimationHeuristicsArr))
  assertIsArray(mentalStateArr, FUNCTION_TYPE, numberOfRewards.mental,
    "makeCSMG(): mentalStateArr: " + arrayToString(mentalStateArr))
  assertIsArray(mentalUtilities, ARRAY_TYPE, numberOfAgents,
    "makeCSMG(): mentalUtilities: " + arrayToString(mentalUtilities))

  let goalCoeffsNumberByAgent =
    computeGoalCoeffsNumber(numberOfRewards.physical, mentalUtilities)


  /** Reward Utilities */
  let rewardUtilityFunctions = assertDefined(gameSetup.rewardUtilityFunctions,
    "makeCSMG(): gameSetup.rewardUtilityFunctions undefined")

  /** Physical rewards structure
   *  This simply delegates to the physicalRewards object as given
   *  in gameSetup
   */
  let getPhysicalRewardStructure = function () {
    return physicalRewardStructure
  }

  /** Mental reward structure uses mental state dynamics provided in
   * @gameSetup to enable computation of mental rewards in any state.
   * Primarily, this is used by an agent to estimate mental state of
   * their opponent.
   * However, it is also used by an agent to estimate their own mental
   * state at some future point, thereby not requiring to compute belief
   * of that agent.
   *
   * @param initialEstimations (@type array of array of distributions,
   * with holes, indexed by agent)
   *  initial mental state estimations of this agent
   * @param selfAgentID
   *  identifier (nonnegative integer, usually 0 or 1) of an agent
   *  identifies the agent for which this reward structure is provided
   */
  let getMentalRewardStructure = function (initialEstimations, selfAgentID) {
    /**
     * @type array of update functions (each of @type (value, agent, state, action) -> newValue)
     */

    /** Represents @selfAgentID computing mental rewards of @ofAgentID.
     * There are four cases:
     * 1) agent is computing their own mental rewards (agentID = ofAgentID)
     * 2) agent is computing other agent's mental rewards (agentID != ofAgentID)
     *
     * a) value of self mental state is a reward
     * b) value of other's mental state is a reward
     *
     * There are two cases:
     * - if @mentalSnapshot is missing, @agentID is computing its own mental rewards
     *   as part of selecting its best action (expected utility)
     * - if @mentalSnapshot is passed, @agentID is computing its opponent's mental
     *   rewards, either as part of selecting an action or updating belief
     * @param state
     * @param mentalSnapshot (optional)
     *  a reference point from which to use mental state dynamics
     *  it's an object { state, values }
     *  (it snapshots mental state values @estimations at @state)
     *  if omitted, initial state with initial estimations serves as snapshot
     *  snapshot is not used when this agent's mental rewards are being computed
     */
    let computeMentalRewards = function (state, ofAgentID, belief, mentalSnapshot) {
      info("computeMentalRewards(): " + selfAgentID + " computing men rews of " +
        ofAgentID)
      // let estimationsDists = computeMentalStateEstimations(state, other(selfAgentID))
      // let estimations = map(expectation, estimationsDists)
      // info("estimations (expns) computed: " + arrayToString(estimations))
      let mentalState = computeMentalState(state, belief)
      info("computeMentalRewards(): mental state computed: " + arrayToString(mentalState))
      /** ofAgentMentalUtility is an array with one elem per mental attitude
       * and that elem is an array which identifies agents whose attitude
       * (this particular one) ofAgentID cares about */
      let ofAgentMentalUtility = mentalUtilities[ofAgentID]
      if (selfAgentID == ofAgentID) {
        info("computeMentalRewards(): " + selfAgentID + " computing their own rewards")
        let estimateRewardsFromAttitude = function (mentalAttitudeIndex, agentArr) {
          let estimateReward = function(overAgentID) {
            if (overAgentID === selfAgentID) return mentalState[mentalAttitudeIndex]
            return expectation(estimation(state, overAgentID, mentalAttitudeIndex))
          }
          map(estimateReward, agentArr)
        }
        let rewardsByAttitudeArr = mapIndexed(estimateRewardsFromAttitude, ofAgentMentalUtility)
        info("computeMentalRewards(): about to join " + arrayToString(rewardsByAttitudeArr))
        return arrayConcat(rewardsByAttitudeArr)
      } else {
        /** now selfAgentID is computing (estimation) ofAgentID's mental rewards
         * this is nested reasoning and we simplify it by assuming that selfAgentID
         * uses
         * * his mental state estimations to estimate ofAgentID's own mental state
         * * estimation from snapshot to estimate ofAgentID's estimations of
         * selfAdgentID's mental state
         * * his own estimations to estimate ofAgentID's estimation of other
         * agents' mental state
         */
        info("computeMentalRewards(): " + selfAgentID + " computing " +
          ofAgentID + "\'s rewards")
        let estimateRewardsFromAttitude = function (mentalAttitudeIndex, agentArr) {
          /** i'th mental attitude, agentArr identifies agents whose
           * mental state we care about */
          let estimateReward = function(overAgentID) {
            /** j identifies agent whose mental state we're trying to
             * estimate */
            if (overAgentID === selfAgentID) {
              return nestedEstimation(state, ofAgentID, mentalAttitudeIndex, mentalSnapshot)
            }
            return expectation(estimation(state, overAgentID, mentalAttitudeIndex))
          }
          return map(estimateReward, agentArr)
        }
        let rewardsByAttitudeArr = mapIndexed(estimateRewardsFromAttitude, ofAgentMentalUtility)
        info("computeMentalRewards(): about to join " + arrayToString(rewardsByAttitudeArr))
        return arrayConcat(rewardsByAttitudeArr)
      }
    }

    /** returns array of values of mental states */
    let computeMentalState = function (state, belief) {
      info("computeMentalState(): state=" + stateToString(state) +
        ", belief: " + beliefToString(belief, selfAgentID, goalCoeffsNumberByAgent))
      let f = function (mentalStateComputeFn) {
        return mentalStateComputeFn(selfAgentID, belief, state)
      }
      let result = map(f, mentalStateArr)
      info("computeMentalState(): returning")
      return result
    }

    /** returns the estimated value of this agent's mental state (identified
     * by *mentalAttitudeIndex*), computed using mental state dynamics
     * relative to a true value at some past state saved in *mentalSnapshot*
     * */
    let nestedEstimation = function(state, estimatingAgentID, mentalAttitudeIndex, mentalSnapshot) {
      info("nestedEstimation(state=" + stateToString(state) + ", index=" +
        mentalAttitudeIndex + ")")
      assertDefined(state,
        "nestedEstimation(): state undefined!")
      assertDefined(mentalAttitudeIndex,
        "nestedEstimation(): mentalAttitudeIndex undefined!")
      assertDefined(mentalSnapshot,
        "nestedEstimation(): mentalSnapshot undefined!")
      assertDefined(mentalSnapshot.state,
        "nestedEstimation(): mentalSnapshot.state undefined!")
      assertDefined(mentalSnapshot.values,
        "nestedEstimation(): mentalSnapshot.values undefined!")
      let snapState = mentalSnapshot.state
      let snapValues = mentalSnapshot.values
      let updateFn = estimationHeuristicsArr[mentalAttitudeIndex]
      /** Local recursive function to save stack space by passing less parameters */
      let nestedEstimationRec = function(state) {
        if (_.isEqual(state, snapState)) {
          return snapValues[mentalAttitudeIndex]
        }
        let prevState = getPreviousState(state)
        let lastAction = getLastAction(state)
        let prevValue = nestedEstimationRec(prevState)
        let curValue = updateFn(prevValue, estimatingAgentID, selfAgentID, prevState, lastAction)
        return curValue
      }
      let result = nestedEstimationRec(state)
      info("nestedEstimation(): returning")
      return result
    }

    // let computeMentalRewardRecFromSnapshot = dp.cache(function (state, mentalSnapshot) {
    //   assertDefined(mentalSnapshot,
    //     "mentalRewardRecFromSnapshot: Mental snapshot undefined!")
    //   let snapState = mentalSnapshot.state
    //   let snapValues = mentalSnapshot.values
    //   if (_.isEqual(state, snapState)) {
    //     return snapValues
    //   }
    //   let prevState = getPreviousState(state)
    //   let lastAction = getLastAction(state)
    //   let prevValues = computeMentalRewardRecFromSnapshot(prevState, mentalSnapshot)
    //   let updateValue = function (updateFn, prevValue) {
    //     return updateFn(prevValue, selfAgentID, prevState, lastAction)
    //   }
    //   let curValues = map2(updateValue, estimationHeuristicsArr, prevValues)
    //   return curValues
    // })

    /** computes this agent's (selfAgentID) estimation of *ofAgentID*
     * mental state (attitude *rewardIndex*) */
    let estimation = dp.cache(
      function(state, ofAgentID, rewardIndex) {
        info("estimation(): state=" + stateToString(state) +
          ", ofAgentID=" + ofAgentID + ", rewardIndex=" + rewardIndex)
        if (isInitial(state)) {
          assert(initialEstimations[ofAgentID] !== undefined &&
            initialEstimations[ofAgentID][rewardIndex] !== undefined,
            "initial estimations of agent " + selfAgentID +
            " about " + ofAgentID + " on mental reward " + rewardIndex +
            " requested, but undefined")
          return initialEstimations[ofAgentID][rewardIndex]
        }
        let prevState = getPreviousState(state)
        let lastAction = getLastAction(state)
        let prevEstimation = estimation(prevState, ofAgentID, rewardIndex)
        let updateEstimation = function (updateFn, prevEstimation) {
          return Infer({method: 'enumerate'}, function () {
            assertHasType(prevEstimation, DIST_TYPE,
              "estimation(): " + selfAgentID + "'s initial " +
              "estimation of " + ofAgentID + "'s mental state " +
              rewardIndex + " is not a distribution: it's " +
              toString(prevEstimation))
            let prevValue = sample(prevEstimation)
            return updateFn(prevValue, selfAgentID, ofAgentID, prevState, lastAction)
          })
        }
        // let curEstimations = map2(updateEstimation, estimationHeuristicsArr, prevEstimations)
        return updateEstimation(estimationHeuristicsArr[rewardIndex], prevEstimation)
      })

    // let computeMentalStateEstimations = dp.cache(function (state, ofAgentID) {
    //   if (isInitial(state)) return initialEstimations[ofAgentID]
    //   let prevState = getPreviousState(state)
    //   let lastAction = getLastAction(state)
    //   let prevEstimations = computeMentalStateEstimations(prevState, ofAgentID)
    //   let updateEstimation = function (updateFn, prevEstimation) {
    //     return Infer({method: 'enumerate'}, function () {
    //       let prevValue = sample(prevEstimation)
    //       return updateFn(prevValue, other(selfAgentID), prevState, lastAction)
    //     })
    //   }
    //   let curEstimations = map2(updateEstimation, estimationHeuristicsArr, prevEstimations)
    //   return curEstimations
    // })

    return {
      computeMentalRewards,
      computeMentalState
    }
  }

  let utilityFn = function (goalCoeffs, rewards) {
    info("utilityFn(): goalCoeffs: " + arrayToString(goalCoeffs) +
      ", rewards: " + rewards)
    assertDefined(goalCoeffs, "utilityFn(): goalCoeffs undefined")
    assertDefined(rewards, "utilityFn(): rewards undefined")
    assertEqual(goalCoeffs.length, rewards.length,
      "Dimension of goal coefficients= " + goalCoeffs.length +
      " doesn't match rewards= " + rewards.length)
    let rewardUtilities = map2(apply, rewardUtilityFunctions, rewards)
    let result = sum(map2(multiply, goalCoeffs, rewardUtilities))
    info("utility computed: " + result)
    return result
  }

  let updatedParams = extend(params, {
    goalCoeffsNumberByAgent,
    beliefRepresentation
  })

  return {
    actions,
    transitionFn,
    initialState,
    API,
    getPhysicalRewardStructure,
    getMentalRewardStructure,
    utilityFn,
    params: updatedParams
  }
}